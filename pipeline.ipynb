{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: ok\n"
     ]
    }
   ],
   "source": [
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_box_markup as show_span_markup, show_ascii_markup as show_markup, show_dep_ascii_markup as show_dep_markup\n",
    "from slovnet import Morph, Syntax\n",
    "from razdel import sentenize, tokenize\n",
    "\n",
    "from yargy import Parser, rule, and_, not_, or_\n",
    "from yargy.interpretation import fact\n",
    "from yargy.predicates import gram, tag, custom, type, in_\n",
    "from yargy.relations import gnc_relation\n",
    "from yargy.pipelines import morph_pipeline\n",
    "\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "pymorphy = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "\n",
    "navec = Navec.load('./data/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "\n",
    "ner = NER.load('./data/slovnet_ner_news_v1.tar')\n",
    "morph = Morph.load('./data/slovnet_morph_news_v1.tar', batch_size=4)\n",
    "syntax = Syntax.load('./data/slovnet_syntax_news_v1.tar')\n",
    "\n",
    "ner.navec(navec)\n",
    "morph.navec(navec)\n",
    "syntax.navec(navec)\n",
    "\n",
    "print(\"load: ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt =\"\"\"–°–æ–≤–ª–∞–¥–µ–ª—å—Ü—ã —Å–µ—Ç–∏ –º–∞–≥–∞–∑–∏–Ω–æ–≤ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–∏–∑–∫–∏—Ö —Ü–µ–Ω Fix Price –ê—Ä—Ç–µ–º –•–∞—á–∞—Ç—Ä—è–Ω –∏ –°–µ—Ä–≥–µ–π –õ–æ–º–∞–∫–∏–Ω —Å—Ç–∞–ª–∏ –¥–æ–ª–ª–∞—Ä–æ–≤—ã–º–∏ –º–∏–ª–ª–∏–∞—Ä–¥–µ—Ä–∞–º–∏ –ø–æ—Å–ª–µ –≤—ã—Ö–æ–¥–∞ –∫–æ–º–ø–∞–Ω–∏–∏ –Ω–∞ IPO (–ø–µ—Ä–≤–∏—á–Ω–æ–µ –ø—É–±–ª–∏—á–Ω–æ–µ —Ä–∞–∑–º–µ—â–µ–Ω–∏–µ –∞–∫—Ü–∏–π). –û–± —ç—Ç–æ–º —Å–≤–∏–¥–µ—Ç–µ–ª—å—Å—Ç–≤—É—é—Ç –ø–æ–¥—Å—á–µ—Ç—ã Forbes.\n",
    "–ñ—É—Ä–Ω–∞–ª –æ—Ü–µ–Ω–∏–ª —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∏–∑ –Ω–∏—Ö –≤ 3,4 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ–ª–ª–∞—Ä–æ–≤. Bloomberg –Ω–∞–ø–∏—Å–∞–ª –æ 3,6 –º–∏–ª–ª–∏–∞—Ä–¥–∞.\n",
    "Fix Price —Ä–∞–∑–º–µ—Å—Ç–∏–ª–∞ —Å–≤–æ–∏ –±—É–º–∞–≥–∏ –Ω–∞ –õ–æ–Ω–¥–æ–Ω—Å–∫–æ–π –±–∏—Ä–∂–µ –ø–æ —Ü–µ–Ω–µ 9,75 –¥–æ–ª–ª–∞—Ä–∞ –∑–∞ —à—Ç—É–∫—É. –õ–æ–º–∞–∫–∏–Ω—É –∏ –•–∞—á–∞—Ç—Ä—è–Ω—É –¥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∞–ª–æ –ø–æ 354 067 500 –∞–∫—Ü–∏–π (41,66%) –∫–æ–º–ø–∞–Ω–∏–π, —Å–µ–π—á–∞—Å ‚Äî –ø–æ 301 151 876 –∞–∫—Ü–∏–π (35,43%). –¢–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, –≤ —Ö–æ–¥–µ IPO –∫–∞–∂–¥—ã–π –∏–∑ –Ω–∏—Ö –ø—Ä–æ–¥–∞–ª –ø–æ 52,9 –º–∏–ª–ª–∏–æ–Ω–∞ –∞–∫—Ü–∏–π, –∏ –∑–∞—Ä–∞–±–æ—Ç–∞–ª –ø–æ 516 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–æ–ª–ª–∞—Ä–æ–≤, –ø–æ–¥—Å—á–∏—Ç–∞–ª Forbes.\n",
    "–†–∞–Ω–µ–µ —Å–µ—Ç—å Fix Price, –±—É–º–∞–≥–∏ –∫–æ—Ç–æ—Ä–æ–π —Å 10 –º–∞—Ä—Ç–∞ 2021 –≥–æ–¥–∞ –Ω–∞—á–Ω—É—Ç —Ç–æ—Ä–≥–æ–≤–∞—Ç—å—Å—è –Ω–∞ –õ–æ–Ω–¥–æ–Ω—Å–∫–æ–π –∏ –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –±–∏—Ä–∂–∞—Ö, —Å–æ–æ–±—â–∏–ª–∞, —á—Ç–æ –ø—Ä–∏–≤–ª–µ—á–µ—Ç –≤ —Ö–æ–¥–µ IPO 2 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ–ª–ª–∞—Ä–æ–≤.\n",
    "¬´–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å¬ª –ø–∏—Å–∞–ª, —á—Ç–æ —ç—Ç–æ –∫—Ä—É–ø–Ω–µ–π—à–µ–µ IPO —Ä–æ—Å—Å–∏–π—Å–∫–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ —Å 2010 –≥–æ–¥–∞, –∫–æ–≥–¥–∞ ¬´–†—É—Å–∞–ª¬ª –≤ —Ä–∞–º–∫–∞—Ö –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø—É–±–ª–∏—á–Ω–æ–≥–æ —Ä–∞–∑–º–µ—â–µ–Ω–∏—è –∞–∫—Ü–∏–π –ø—Ä–∏–≤–ª–µ–∫ 2,24 –º–∏–ª–ª–∏–∞—Ä–¥–∞ –¥–æ–ª–ª–∞—Ä–æ–≤.\n",
    "–ü–µ—Ä–≤—ã–µ –º–∞–≥–∞–∑–∏–Ω—ã —Å–µ—Ç–∏ Fix Price –±—ã–ª–∏ –æ—Ç–∫—Ä—ã—Ç—ã –≤ 2007 –≥–æ–¥—É. –°–µ–π—á–∞—Å —É –∫–æ–º–ø–∞–Ω–∏–∏ 4279 –º–∞–≥–∞–∑–∏–Ω–æ–≤ –≤ –†–æ—Å—Å–∏–∏, –ë–µ–ª–∞—Ä—É—Å–∏, –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–µ, –£–∑–±–µ–∫–∏—Å—Ç–∞–Ω–µ, –ö—ã—Ä–≥—ã–∑—Å—Ç–∞–Ω–µ, –ì—Ä—É–∑–∏–∏ –∏ –õ–∞—Ç–≤–∏–∏. –í–Ω–∞—á–∞–ª–µ –≤–µ—Å—å –∞—Å—Å–æ—Ä—Ç–∏–º–µ–Ω—Ç –≤ Fix Price –ø—Ä–æ–¥–∞–≤–∞–ª—Å—è –ø–æ 30 —Ä—É–±–ª–µ–π –∑–∞ —Ç–æ–≤–∞—Ä, —Å–µ–π—á–∞—Å —Ç–æ–≤–∞—Ä—ã —Ç–∞–º —Å—Ç–æ—è—Ç –Ω–µ –¥–æ—Ä–æ–∂–µ 250 —Ä—É–±–ª–µ–π.\n",
    "–ê—Ä—Ç–µ–º –•–∞—á–∞—Ç—Ä—è–Ω –∏ –°–µ—Ä–≥–µ–π –õ–æ–º–∞–∫–∏–Ω –≤ 1998 –≥–æ–¥—É —Å—Ç–∞–ª–∏ —Å–æ–æ—Å–Ω–æ–≤–∞—Ç–µ–ª—è–º–∏ —Å–µ—Ç–∏ –¥–∏—Å–∫–∞—É–Ω—Ç–µ—Ä–æ–≤ ¬´–ö–æ–ø–µ–π–∫–∞¬ª (—Å–µ–π—á–∞—Å —ç—Ç–∏ –º–∞–≥–∞–∑–∏–Ω—ã –∏–∑–≤–µ—Å—Ç–Ω—ã –ø–æ–¥ –±—Ä–µ–Ω–¥–æ–º ¬´–ü—è—Ç–µ—Ä–æ—á–∫–∞¬ª). –í 2007 –≥–æ–¥—É –æ–Ω–∏ –ø—Ä–æ–¥–∞–ª–∏ —Å–≤–æ—é –¥–æ–ª—é –∫–æ—Ä–ø–æ—Ä–∞—Ü–∏–∏ ¬´–£—Ä–∞–ª—Å–∏–±¬ª –∑–∞ 220 –º–∏–ª–ª–∏–æ–Ω–æ–≤ –¥–æ–ª–ª–∞—Ä–æ–≤. –¢–∞–∫–∂–µ –•–∞—á–∞—Ç—Ä—è–Ω –∏ –õ–æ–º–∞–∫–∏–Ω –∏–Ω–≤–µ—Å—Ç–∏—Ä–æ–≤–∞–ª–∏ –≤ —Å–µ—Ç–∏ ¬´–ú–æ–¥–∏—Å¬ª –∏ ¬´–¶–µ–Ω—Ç—Ä–û–±—É–≤—å¬ª.\n",
    "–í –º–∞–µ 2020 –≥–æ–¥–∞ Forbes –ø–∏—Å–∞–ª, —á—Ç–æ –≤ –†–æ—Å—Å–∏–∏ –±–æ–ª—å—à–µ 100 –¥–æ–ª–ª–∞—Ä–æ–≤—ã—Ö –º–∏–ª–ª–∏–∞—Ä–¥–µ—Ä–æ–≤.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–ù–∞ —Å–∞–π—Ç–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω–æ–≥–æ —Ü–µ–Ω—Ç—Ä–∞ –ù–ê–¢–û ¬´–ê—Ç–ª–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç¬ª –≤—ã—à–ª–∞ –Ω–µ—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–∞—è –¥–ª—è –Ω–µ–≥–æ —Å—Ç–∞—Ç—å—è.\n",
    "–í –Ω–µ–π –∞–≤—Ç–æ—Ä—ã —Ä–µ–∫–æ–º–µ–Ω–¥—É—é—Ç —Å—Ç—Ä–∞–Ω–∞–º –≤–æ–µ–Ω–Ω–æ–≥–æ –∞–ª—å—è–Ω—Å–∞ –ø—Ä–∏–º–µ–Ω—è—Ç—å –∫ –†–æ—Å—Å–∏–∏ –±–æ–ª–µ–µ –≥–∏–±–∫—É—é –≤–Ω–µ—à–Ω—é—é –ø–æ–ª–∏—Ç–∏–∫—É, –æ—Å–Ω–æ–≤–∞–Ω–Ω—É—é –Ω–µ –Ω–∞ –æ—á–µ—Ä–µ–¥–Ω—ã—Ö —Å–∞–Ω–∫—Ü–∏—è—Ö –∑–∞ –Ω–∞—Ä—É—à–µ–Ω–∏—è –ø—Ä–∞–≤ —á–µ–ª–æ–≤–µ–∫–∞, –∞ –Ω–∞ –≤–∑–∞–∏–º–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏—á–µ—Å–∫–∏—Ö –∏–Ω—Ç–µ—Ä–µ—Å–∞—Ö.\n",
    "–ê –¥–µ–º–æ–∫—Ä–∞—Ç–∏–∑–∞—Ü–∏—è –†–æ—Å—Å–∏–∏, –≥–æ–≤–æ—Ä–∏—Ç—Å—è –≤ —Å—Ç–∞—Ç—å–µ, –≤–æ–æ–±—â–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ–≤—ã–≥–æ–¥–Ω–∞ –°–®–ê, –≤–µ–¥—å —Ç–æ—Ç –∂–µ –ê–ª–µ–∫—Å–µ–π –ù–∞–≤–∞–ª—å–Ω—ã–π ‚Äî –Ω–∞—Ü–∏–æ–Ω–∞–ª–∏—Å—Ç –∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–ª –ö—Ä–µ–º–ª—å –≤ –∫—Ä—ã–º—Å–∫–æ–º –≤–æ–ø—Ä–æ—Å–µ.\n",
    "–°—Ç–∞—Ç—å—è –≤—ã–∑–≤–∞–ª–∞ –≥—Ä–æ–º–∫–∏–π —Å–∫–∞–Ω–¥–∞–ª –≤ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –∫—Ä—É–≥–∞—Ö –∏ –æ—Ç–≤–µ—Ç–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –ø–∏—Å—å–º–∞ —Å –æ–±–≤–∏–Ω–µ–Ω–∏—è–º–∏ –∞–≤—Ç–æ—Ä–æ–≤ –≤ –∫–æ—Ä—Ä—É–ø—Ü–∏–∏.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–ö–æ–º–ø–∞–Ω–∏—è ¬´–Ø–Ω–¥–µ–∫—Å¬ª –≤–µ–¥–µ—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã –æ¬†–ø–æ–∫—É–ø–∫–µ –Ω–µ–±–æ–ª—å—à–æ–≥–æ –±–∞–Ω–∫–∞ ¬´–ê–∫—Ä–æ–ø–æ–ª—å¬ª, \n",
    "–∫–æ—Ç–æ—Ä—ã–π –ø—Ä–∏–Ω–∞–¥–ª–µ–∂–∏—Ç –≥–µ–Ω–µ—Ä–∞–ª—å–Ω–æ–º—É –¥–∏—Ä–µ–∫—Ç–æ—Ä—É –≥—Ä—É–ø–ø—ã ¬´–°–≤—è–∑–Ω–æ–π¬ª –ï–≤–≥–µ–Ω–∏—é \n",
    "–î–∞–≤—ã–¥–æ–≤–∏—á—É. –û–±¬†—ç—Ç–æ–º —Å–æ–æ–±—â–∞–µ—Ç The Bell —Å–æ¬†—Å—Å—ã–ª–∫–æ–π –Ω–∞¬†—á–µ—Ç—ã—Ä–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ \n",
    "–Ω–∞¬†–ø–ª–∞—Ç–µ–∂–Ω–æ–º –∏¬†–±–∞–Ω–∫–æ–≤—Å–∫–æ–º —Ä—ã–Ω–∫–µ.¬†\n",
    "–°–¥–µ–ª–∫–∞, –ø–æ¬†–¥–∞–Ω–Ω—ã–º –∏–∑–¥–∞–Ω–∏—è, —É–∂–µ –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –Ω–∞¬†–æ–¥–Ω–æ–º –∏–∑¬†–∑–∞–≤–µ—Ä—à–∞—é—â–∏—Ö —ç—Ç–∞–ø–æ–≤. \n",
    "–î–æ–≥–æ–≤–æ—Ä –∫—É–ø–ª–∏-–ø—Ä–æ–¥–∞–∂–∏ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–¥–ø–∏—Å–∞–Ω –≤¬†–±–ª–∏–∂–∞–π—à–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π. –°—É–º–º—É\n",
    "—Å–¥–µ–ª–∫–∏ The Bell –Ω–µ¬†–ø—Ä–∏–≤–æ–¥–∏—Ç.¬†\n",
    "–ê–∫—Ç–∏–≤—ã ¬´–ê–∫—Ä–æ–ø–æ–ª—è¬ª —Å–æ—Å—Ç–∞–≤–ª—è—é—Ç 1,24 –º–∏–ª–ª–∏–∞—Ä–¥–∞ —Ä—É–±–ª–µ–π. –í¬†—Ä–µ–π—Ç–∏–Ω–≥–µ –∫—Ä—É–ø–Ω–µ–π—à–∏—Ö \n",
    "–±–∞–Ω–∫–æ–≤ ¬´–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å-100¬ª –æ–Ω¬†–∑–∞–Ω–∏–º–∞–µ—Ç 323-—é —Å—Ç—Ä–æ—á–∫—É.\n",
    "–ï–≤–≥–µ–Ω–∏–π –î–∞–≤—ã–¥–æ–≤–∏—á –æ—Ç–∫–∞–∑–∞–ª—Å—è –æ—Ç¬†–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤. –ü—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç–µ–ª—å ¬´–Ø–Ω–¥–µ–∫—Å–∞¬ª \n",
    "—Å–æ–æ–±—â–∏–ª The Bell, —á—Ç–æ –∫–æ–º–ø–∞–Ω–∏–∏ ¬´–∏–Ω—Ç–µ—Ä–µ—Å–Ω–æ –ø–æ–ª—É—á–µ–Ω–∏–µ –±–∞–Ω–∫–æ–≤—Å–∫–æ–π –ª–∏—Ü–µ–Ω–∑–∏–∏¬ª. \n",
    "¬´–ú—ã¬†—Ä–∞—Å—Å–º–∞—Ç—Ä–∏–≤–∞–µ–º –≤—Å–µ –¥–æ—Å—Ç—É–ø–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã¬ª,¬†‚Äî –¥–æ–±–∞–≤–∏–ª¬†–æ–Ω.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–†–µ–∂–∏—Å—Å–µ—Ä —Ñ–∏–ª—å–º–∞ ¬´–°—É–ø–µ—Ä–Ω–æ–≤–∞¬ª –ì–∞—Ä—Ä–∏ –ú–∞–∫–∫–≤–∏–Ω –ø–æ–¥–≤–µ—Ä–≥ –∫—Ä–∏—Ç–∏–∫–µ –¥–µ–π—Å—Ç–≤–∏—è \n",
    "–ø—Ä–æ–∫–∞—Ç—á–∏–∫–∞ –∫–∞—Ä—Ç–∏–Ω—ã –≤¬†–†–æ—Å—Å–∏–∏, –≤—ã—Ä–µ–∑–∞–≤—à–µ–≥–æ —Å—Ü–µ–Ω—É, –≥–¥–µ –≥–µ–π-–ø–∞—Ä–∞ –ø—ã—Ç–∞–µ—Ç—Å—è –∑–∞–Ω—è—Ç—å—Å—è \n",
    "—Å–µ–∫—Å–æ–º. –ó–∞—è–≤–ª–µ–Ω–∏–µ —Ä–µ–∂–∏—Å—Å–µ—Ä–∞ –ø—Ä–∏–≤–æ–¥–∏—Ç –∏–∑–¥–∞–Ω–∏–µ The Advocate.¬†\n",
    "–ú—ã, —Å–æ–∑–¥–∞—Ç–µ–ª–∏ —Ñ–∏–ª—å–º–∞, —Å–∞–º—ã–º —Ä–µ—à–∏—Ç–µ–ª—å–Ω—ã–º –æ–±—Ä–∞–∑–æ–º –≤–æ–∑—Ä–∞–∂–∞–µ–º –ø—Ä–æ—Ç–∏–≤ —Ü–µ–Ω–∑—É—Ä—ã\n",
    "¬´–°—É–ø–µ—Ä–Ω–æ–≤—ã¬ª –≤¬†–†–æ—Å—Å–∏–∏. –í—ã–∑—ã–≤–∞–µ—Ç –≥–ª—É–±–æ–∫—É—é –æ–±–µ—Å–ø–æ–∫–æ–µ–Ω–Ω–æ—Å—Ç—å —Ñ–∞–∫—Ç —Ç–æ–≥–æ, —á—Ç–æ \n",
    "—Ñ–∏–ª—å–º –±—ã–ª —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –ø—Ä–æ—Ç–∏–≤ –Ω–∞—à–µ–π –≤–æ–ª–∏ –∏¬†–±–µ–∑ –Ω–∞—à–µ–≥–æ —Ä–∞–∑—Ä–µ—à–µ–Ω–∏—è.\n",
    "–ú–∞–∫–∫–≤–∏–Ω –ø–æ–¥—á–µ—Ä–∫–Ω—É–ª, —á—Ç–æ –∫–æ–º–∞–Ω–¥–∞ —Å–æ–∑–¥–∞—Ç–µ–ª–µ–π —Ñ–∏–ª—å–º–∞ –ø–æ–Ω–∏–º–∞–µ—Ç, —á—Ç–æ \n",
    "–Ω–∞¬†–¥–∏—Å—Ç—Ä–∏–±—å—é—Ç–æ—Ä–∞ –æ–∫–∞–∑—ã–≤–∞–ª–æ—Å—å –¥–∞–≤–ª–µ–Ω–∏–µ, –Ω–æ¬†¬´–Ω–µ¬†–ø–æ—Ç–µ—Ä–ø–∏—Ç —Ü–µ–Ω–∑—É—Ä—ã —Ç–∞–∫–æ–≥–æ —Ä–æ–¥–∞¬ª. \n",
    "–í¬†—Å–∞–º–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ World Pictures, –∑–∞–Ω–∏–º–∞—é—â–µ–π—Å—è —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ–º —Ñ–∏–ª—å–º–∞ \n",
    "–≤¬†–†–æ—Å—Å–∏–∏, –æ—Ç¬†–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–µ–≤ –æ—Ç–∫–∞–∑–∞–ª–∏—Å—å.¬†\n",
    "–§–∏–ª—å–º ¬´–°—É–ø–µ—Ä–Ω–æ–≤–∞¬ª —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ¬†–∂–∏–∑–Ω–∏ –≥–µ–π-–ø–∞—Ä—ã, –≤¬†–∫–æ—Ç–æ—Ä–æ–π –æ–¥–∏–Ω \n",
    "–∏–∑¬†–ø–∞—Ä—Ç–Ω–µ—Ä–æ–≤ —Ç–µ—Ä—è–µ—Ç –ø–∞–º—è—Ç—å –∏–∑-–∑–∞ –¥–µ–º–µ–Ω—Ü–∏–∏. –ò–∑¬†—Ä–æ—Å—Å–∏–π—Å–∫–æ–π –≤–µ—Ä—Å–∏–∏ –∫–∞—Ä—Ç–∏–Ω—ã –±—ã–ª \n",
    "–≤—ã—Ä–µ–∑–∞–Ω —Ç—Ä–µ—Ö–º–∏–Ω—É—Ç–Ω—ã–π —ç–ø–∏–∑–æ–¥, –≤¬†–∫–æ—Ç–æ—Ä–æ–º –º—É–∂—á–∏–Ω—ã –ø—ã—Ç–∞—é—Ç—Å—è –∑–∞–Ω—è—Ç—å—Å—è —Å–µ–∫—Å–æ–º.¬†\n",
    "–í¬†2019 –≥–æ–¥—É –≤¬†—Ä–æ—Å—Å–∏–π—Å–∫–æ–º –ø—Ä–æ–∫–∞—Ç–µ –ø–æ—Ö–æ–∂–µ–π —Ü–µ–Ω–∑—É—Ä–µ –±—ã–ª –ø–æ–¥–≤–µ—Ä–≥–Ω—É—Ç —Ñ–∏–ª—å–º \n",
    "¬´–†–æ–∫–µ—Ç–º–µ–Ω¬ª, —Å–Ω—è—Ç—ã–π –ø–æ¬†–º–æ—Ç–∏–≤–∞–º –±–∏–æ–≥—Ä–∞—Ñ–∏–∏ –ø–µ–≤—Ü–∞ –≠–ª—Ç–æ–Ω–∞ –î–∂–æ–Ω–∞. –í¬†–†–§¬†—Ñ–∏–ª—å–º \n",
    "–ø–æ–∫–∞–∑—ã–≤–∞–ª—Å—è –±–µ–∑ —Å—Ü–µ–Ω—ã —Å–µ–∫—Å–∞ –¥–≤—É—Ö –º—É–∂—á–∏–Ω –∏¬†—Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö —Ç–∏—Ç—Ä–æ–≤, –≤¬†–∫–æ—Ç–æ—Ä—ã—Ö \n",
    "–≥–æ–≤–æ—Ä–∏–ª–æ—Å—å, —á—Ç–æ —Å–µ–π—á–∞—Å –ø–µ–≤–µ—Ü –∂–∏–≤–µ—Ç –≤¬†–±—Ä–∞–∫–µ —Å¬†–º—É–∂—á–∏–Ω–æ–π.¬†–°–æ–∑–¥–∞—Ç–µ–ª–∏ —Ñ–∏–ª—å–º–∞ \n",
    "–∏¬†—Å–∞–º –≠–ª—Ç–æ–Ω –î–∂–æ–Ω –ø–æ–¥–≤–µ—Ä–≥–ª–∏ —ç—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –∫—Ä–∏—Ç–∏–∫–µ.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–ú–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã–π —Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä–∞–∂ (CAS) –≤¬†–õ–æ–∑–∞–Ω–Ω–µ –æ—Ç–∫–ª–æ–Ω–∏–ª –∑–∞–ø—Ä–æ—Å \n",
    "–û–ª–∏–º–ø–∏–π—Å–∫–æ–≥–æ –∫–æ–º–∏—Ç–µ—Ç–∞ –†–æ—Å—Å–∏–∏, –ø—Ä–µ–¥–ª–∞–≥–∞–≤—à–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–µ—Å–Ω—é ¬´–ö–∞—Ç—é—à–∞¬ª \n",
    "–≤–º–µ—Å—Ç–æ —Ä–æ—Å—Å–∏–π—Å–∫–æ–≥–æ –≥–∏–º–Ω–∞ –Ω–∞¬†–û–ª–∏–º–ø–∏–π—Å–∫–∏—Ö –∏–≥—Ä–∞—Ö –≤¬†–¢–æ–∫–∏–æ –∏¬†–ü–µ–∫–∏–Ω–µ.\n",
    "¬´–ö–æ–º–∏—Å—Å–∏—è CAS —Å—á–∏—Ç–∞–µ—Ç, —á—Ç–æ [–ø–æ–Ω—è—Ç–∏–µ] ‚Äû–ª—é–±–æ–π –≥–∏–º–Ω, —Å–≤—è–∑–∞–Ω–Ω—ã–π —Å¬†–†–æ—Å—Å–∏–µ–π‚Äú, \n",
    "—Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –Ω–∞¬†–ª—é–±—É—é –ø–µ—Å–Ω—é, —Å–≤—è–∑–∞–Ω–Ω—É—é —Å¬†–†–æ—Å—Å–∏–µ–π –∏–ª–∏ —Å–æ¬†—Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞¬†–Ω–µ–µ, \n",
    "–≤¬†—Ç–æ–º —á–∏—Å–ª–µ —Å¬†‚Äû–ö–∞—Ç—é—à–µ–π‚Äú¬ª,¬†‚Äî —Å–∫–∞–∑–∞–Ω–æ –≤¬†—Ä–µ—à–µ–Ω–∏–∏ —Å—É–¥–∞, –∫–æ—Ç–æ—Ä–æ–µ¬†—Ü–∏—Ç–∏—Ä—É–µ—Ç Associated \n",
    "Press.\n",
    "–í¬†–¥–µ–∫–∞–±—Ä–µ 2019 –≥–æ–¥–∞ –í—Å–µ–º–∏—Ä–Ω–æ–µ –∞–Ω—Ç–∏–¥–æ–ø–∏–Ω–≥–æ–≤–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ –æ—Ç—Å—Ç—Ä–∞–Ω–∏–ª–æ –†–æ—Å—Å–∏—é \n",
    "–æ—Ç¬†–û–ª–∏–º–ø–∏–π—Å–∫–∏—Ö –∏¬†–ü–∞—Ä–∞–ª–∏–º–ø–∏–π—Å–∫–∏—Ö¬†–∏–≥—Ä, –∞¬†—Ç–∞–∫–∂–µ —á–µ–º–ø–∏–æ–Ω–∞—Ç–æ–≤ –º–∏—Ä–∞ –∏–∑-–∑–∞ \n",
    "–º–∞–Ω–∏–ø—É–ª—è—Ü–∏–π —Å¬†–¥–æ–ø–∏–Ω–≥-–ø—Ä–æ–±–∞–º–∏, –∫–æ—Ç–æ—Ä—ã–µ —Å–æ–¥–µ—Ä–∂–∞–ª–∏—Å—å –≤¬†–±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö \n",
    "–ú–æ—Å–∫–æ–≤—Å–∫–æ–π –∞–Ω—Ç–∏–¥–æ–ø–∏–Ω–≥–æ–≤–æ–π –ª–∞–±–æ—Ä–∞—Ç–æ—Ä–∏–∏.\n",
    "–í—Å–µ–º–∏—Ä–Ω–æ–µ –∞–Ω—Ç–∏–¥–æ–ø–∏–Ω–≥–æ–≤–æ–µ –∞–≥–µ–Ω—Ç—Å—Ç–≤–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–ª–æ –æ—Ç—Å—Ç—Ä–∞–Ω–∏—Ç—å –†–æ—Å—Å–∏—é \n",
    "–æ—Ç¬†–º–µ–∂–¥—É–Ω–∞—Ä–æ–¥–Ω—ã—Ö —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏–π –Ω–∞¬†—á–µ—Ç—ã—Ä–µ –≥–æ–¥–∞, –Ω–æ¬†–≤¬†–¥–µ–∫–∞–±—Ä–µ 2020 –≥–æ–¥–∞ \n",
    "—Å–ø–æ—Ä—Ç–∏–≤–Ω—ã–π –∞—Ä–±–∏—Ç—Ä–∞–∂ —Ä–µ—à–∏–ª —Å–æ–∫—Ä–∞—Ç–∏—Ç—å —Å—Ä–æ–∫ –Ω–∞–∫–∞–∑–∞–Ω–∏—è –¥–æ¬†–¥–≤—É—Ö¬†–ª–µ—Ç. –¢–∞–∫–∏–º \n",
    "–æ–±—Ä–∞–∑–æ–º, —Ä–æ—Å—Å–∏–π—Å–∫–∏–µ —Å–ø–æ—Ä—Ç—Å–º–µ–Ω—ã –Ω–µ¬†—Å–º–æ–≥—É—Ç –¥–æ¬†16¬†–¥–µ–∫–∞–±—Ä—è 2022 –≥–æ–¥–∞ –≤—ã—Å—Ç—É–ø–∞—Ç—å \n",
    "–ø–æ–¥ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–º —Ñ–ª–∞–≥–æ–º, –∞¬†—Ç–∞–∫–∂–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –≥–∏–º–Ω –∏¬†–Ω–∞–∑–≤–∞–Ω–∏–µ \n",
    "—Å–±–æ—Ä–Ω–æ–π –†–æ—Å—Å–∏–∏.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–§–æ—Ä—É–º –ø—Ä–æ–≤–æ–¥–∏—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è ¬´–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–µ –¥–µ–º–æ–∫—Ä–∞—Ç—ã¬ª, –∫–æ—Ç–æ—Ä–∞—è \n",
    "–Ω–µ¬†—è–≤–ª—è–µ—Ç—Å—è ¬´–Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–π¬ª. –ü–æ¬†–¥–∞–Ω–Ω—ã–º ¬´–î–æ–∂–¥—è¬ª, –∑–∞–¥–µ—Ä–∂–∞–Ω–∏—è –º–æ—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω—ã \n",
    "–¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å—é ¬´–û—Ç–∫—Ä—ã—Ç–æ–π –†–æ—Å—Å–∏–∏¬ª, –∫–æ—Ç–æ—Ä–∞—è, –ø–æ¬†–º–µ–Ω—å—à–µ–π –º–µ—Ä–µ —Ñ–æ—Ä–º–∞–ª—å–Ω–æ, \n",
    "–Ω–µ¬†–∏–º–µ–µ—Ç –æ—Ç–Ω–æ—à–µ–Ω–∏—è –∫¬†–ø—Ä–æ–≤–µ–¥–µ–Ω–∏—é —Ñ–æ—Ä—É–º–∞. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤¬†–í–µ–ª–∏–∫–æ–±—Ä–∏—Ç–∞–Ω–∏–∏ \n",
    "–æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏—è —Å¬†—Ç–∞–∫–∏–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º –±—ã–ª–∞ –µ—â–µ –≤¬†2017 –≥–æ–¥—É –ø—Ä–∏–∑–Ω–∞–Ω–∞ \n",
    "–≤¬†–†–§¬†¬´–Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–π¬ª, –ø–æ—Å–ª–µ —á–µ–≥–æ –Ω–∞—á–∞–ª–æ—Å—å –ø—Ä–µ—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ —á–ª–µ–Ω–æ–≤ –æ–¥–Ω–æ–∏–º–µ–Ω–Ω–æ–≥–æ \n",
    "–¥–≤–∏–∂–µ–Ω–∏—è ¬´–û—Ç–∫—Ä—ã—Ç–∞—è –†–æ—Å—Å–∏—è¬ª. –í¬†2019 –≥–æ–¥—É –æ–Ω–æ –æ–±—ä—è–≤–∏–ª–æ –æ¬†–ª–∏–∫–≤–∏–¥–∞—Ü–∏–∏, –∏¬†–∞–∫—Ç–∏–≤–∏—Å—Ç—ã\n",
    "—ç—Ç–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —É—á—Ä–µ–¥–∏–ª–∏ –¥—Ä—É–≥—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Å¬†–∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã–º –Ω–∞–∑–≤–∞–Ω–∏–µ–º, \n",
    "–Ω–æ¬†–Ω–µ¬†–ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—é –≤¬†–ú–∏–Ω—é—Å—Ç–µ. ¬´–û–±—ã—á–Ω–æ, —Ä–∞–Ω—å—à–µ –ø–æ¬†–∫—Ä–∞–π–Ω–µ–π –º–µ—Ä–µ, \n",
    "–ø—Ä–∏—Ö–æ–¥–∏–ª–∏ –ø–æ¬†—Å—Ç–∞—Ç—å–µ –æ¬†—Å–æ—Ç—Ä—É–¥–Ω–∏—á–µ—Å—Ç–≤–µ —Å¬†–Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–µ–π \n",
    "–Ω–∞¬†–º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è ‚Äû–û—Ç–∫—Ä—ã—Ç–æ–π –†–æ—Å—Å–∏–∏‚Äú. –ö–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º –ø–æ–ª–∏—Ü–∏—è —Å–≤—è–∑–∞–ª–∞ ‚Äû–û—Ç–∫—Ä—ã—Ç–∫—É‚Äú \n",
    "–∏¬†‚Äû–û–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã—Ö –¥–µ–º–æ–∫—Ä–∞—Ç–æ–≤‚Äú, —è¬†–Ω–µ¬†–∑–Ω–∞—é. –≠—Ç–æ –∞–±—Å–æ–ª—é—Ç–Ω–æ —Ä–∞–∑–Ω—ã–µ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏¬ª,¬†‚Äî \n",
    "—Å–∫–∞–∑–∞–ª–∞ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–æ—Ä ¬´–û—Ç–∫—Ä—ã—Ç–æ–π –†–æ—Å—Å–∏–∏¬ª –¢–∞—Ç—å—è–Ω–∞ –£—Å–º–∞–Ω–æ–≤–∞.¬†\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–ü–µ—Ä–≤—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –æ¬†–Ω–µ–≤–∑–∞–∏–º–æ–∑–∞–º–µ–Ω—è–µ–º—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö, –æ–Ω–∏¬†–∂–µ NFT, –ø–æ—è–≤–∏–ª–∏—Å—å –≤¬†2017 \n",
    "–≥–æ–¥—É¬†‚Äî —Ç–æ–≥–¥–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, –ø—Ä–æ–µ–∫—Ç Larva Labs –∑–∞–ø—É—Å—Ç–∏–ª —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –ø–æ–¥ –Ω–∞–∑–≤–∞–Ω–∏–µ–º \n",
    "CryptoPunks. –û–Ω¬†–ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–ª —Å–æ–±–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏—é –∏–∑¬†10 —Ç—ã—Å—è—á —Ü–∏—Ñ—Ä–æ–≤—ã—Ö –∞–≤–∞—Ç–∞—Ä–æ–≤¬†‚Äî \n",
    "–ø–æ¬†—Å—É—Ç–∏, –≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π —Ä–∞–∑–Ω—ã—Ö –ª–∏—Ü, –≤—Å—è–∫–æ–µ –∏–∑¬†–∫–æ—Ç–æ—Ä—ã—Ö —É–Ω–∏–∫–∞–ª—å–Ω–æ, \n",
    "—Ç–æ¬†–µ—Å—Ç—å –Ω–∞–π—Ç–∏ –¥–≤–µ –ø–æ—Ö–æ–∂–∏–µ –∫–∞—Ä—Ç–∏–Ω–∫–∏ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ. –ü—Ä–∏ —ç—Ç–æ–º –∫–∞–∂–¥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ \n",
    "¬´–ø—Ä–∏–≤—è–∑–∞–ª–∏¬ª –∫¬†—Ñ—Ä–∞–≥–º–µ–Ω—Ç—É –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–≥–æ –∫–æ–¥–∞ –≤¬†–±–ª–æ–∫—á–µ–π–Ω-–ø–ª–∞—Ç—Ñ–æ—Ä–º–µ Ethereum¬†‚Äî \n",
    "—Ç–∞–∫–∏–º –æ–±—Ä–∞–∑–æ–º, —É¬†–Ω–µ–≥–æ –±—ã–ª —É–Ω–∏–∫–∞–ª—å–Ω—ã–π —Ç–æ–∫–µ–Ω, –≤¬†–∫–æ—Ç–æ—Ä–æ–º, –≤¬†—á–∞—Å—Ç–Ω–æ—Å—Ç–∏, —Ö—Ä–∞–Ω–∏–ª–∞—Å—å \n",
    "–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ¬†–≤–ª–∞–¥–µ–ª—å—Ü–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.\n",
    "–í¬†–∏—Ç–æ–≥–µ —Å–∫–∞—á–∞—Ç—å —ç—Ç—É –∫–∞—Ä—Ç–∏–Ω–∫—É, –∫–∞–∫ –∏¬†–ª—é–±–æ–µ –¥—Ä—É–≥–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≤¬†–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ, –º–æ–≥ –∫—Ç–æ \n",
    "—É–≥–æ–¥–Ω–æ, –æ–¥–Ω–∞–∫–æ –µ–µ¬†–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω—ã–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–º –≤–ª–∞–¥–µ–ª—å—Ü–µ–º —è–≤–ª—è–ª—Å—è –∏–º–µ–Ω–Ω–æ —Ç–æ—Ç, —á–µ–π\n",
    "Ethereum-–∫–æ—à–µ–ª–µ–∫ –±—ã–ª —É–∫–∞–∑–∞–Ω –≤¬†NFT –∫¬†—ç—Ç–æ–π –∫–∞—Ä—Ç–∏–Ω–∫–µ.¬†\n",
    "–í¬†—á–µ–º —Å–º—ã—Å–ª –æ–±–ª–∞–¥–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ–º, –∫–æ—Ç–æ—Ä–æ–µ –ª—é–±–æ–π —á–µ–ª–æ–≤–µ–∫ –º–æ–∂–µ—Ç \n",
    "–±–µ—Å–ø—Ä–µ–ø—è—Ç—Å—Ç–≤–µ–Ω–Ω–æ —Å–∫–∞—á–∞—Ç—å –≤¬†–∏–Ω—Ç–µ—Ä–Ω–µ—Ç–µ –∏, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–¥–µ–ª–∞—Ç—å —Å–≤–æ–µ–π –∞–≤–∞—Ç–∞—Ä–∫–æ–π \n",
    "–≤¬†—Å–æ—Ü—Å–µ—Ç—è—Ö? –ü–æ–Ω—è—Ç—å —ç—Ç–æ –º–æ–∂–Ω–æ, –ª–∏—à—å —Ä–∞—Å—Å–º–æ—Ç—Ä–µ–≤ —ç—Ç–∏ –∞–≤–∞—Ç–∞—Ä—ã –∫–∞–∫ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è \n",
    "–∏—Å–∫—É—Å—Å—Ç–≤–∞ –∏, –≤¬†—á–∞—Å—Ç–Ω–æ—Å—Ç–∏, –ø—Ä–æ–≤–µ–¥—è –ø–∞—Ä–∞–ª–ª–µ–ª–∏ –º–µ–∂–¥—É –Ω–∏–º–∏ –∏¬†¬´–º–∞—Ç–µ—Ä–∏–∞–ª—å–Ω—ã–º–∏¬ª \n",
    "–æ–±—ä–µ–∫—Ç–∞–º–∏ –≤—Ä–æ–¥–µ –∫–∞—Ä—Ç–∏–Ω –∏–ª–∏ —Å–∫—É–ª—å–ø—Ç—É—Ä. –†–µ–ø—Ä–æ–¥—É–∫—Ü–∏—é ¬´–î–µ–≤–æ—á–∫–∏ —Å¬†–ø–µ—Ä—Å–∏–∫–∞–º–∏¬ª —Ç–æ–∂–µ \n",
    "–º–æ–∂–µ—Ç –ø–æ–≤–µ—Å–∏—Ç—å —É¬†—Å–µ–±—è –Ω–∞¬†–∫—É—Ö–Ω–µ –ª—é–±–æ–π —á–µ–ª–æ–≤–µ–∫, –Ω–æ¬†—Å—É—â–µ—Å—Ç–≤—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª, –∫–æ—Ç–æ—Ä—ã–π \n",
    "—Ö—Ä–∞–Ω–∏—Ç—Å—è –≤¬†–¢—Ä–µ—Ç—å—è–∫–æ–≤—Å–∫–æ–π –≥–∞–ª–µ—Ä–µ–µ, –∏¬†—É¬†–Ω–µ–≥–æ –∏–º–µ–µ—Ç—Å—è –≤–ª–∞–¥–µ–ª–µ—Ü. –ß–µ–º –ø–æ–ø—É–ª—è—Ä–Ω–µ–µ \n",
    "—Å—Ç–∞–Ω–æ–≤–∏—Ç—Å—è –∫–∞—Ä—Ç–∏–Ω–∞ (–≤¬†—Ç–æ–º —á–∏—Å–ª–µ, —á–µ–º –±–æ–ª—å—à–µ –µ–µ¬†—Ä–µ–ø—Ä–æ–¥—É–∫—Ü–∏–π ¬´–ø–æ–π–¥–µ—Ç –≤¬†–Ω–∞—Ä–æ–¥¬ª), \n",
    "—Ç–µ–º –≤—ã—à–µ –º–æ–∂–µ—Ç –±—ã—Ç—å —Ü–µ–Ω–∞ –Ω–∞¬†–æ—Ä–∏–≥–∏–Ω–∞–ª.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–í—ã–≥–æ—Ä–∞–Ω–∏–µ üî•üö¨\n",
    "\n",
    "–°–µ–≥–æ–¥–Ω—è –æ–±—â–∞–ª–∏—Å—å —Å –∫–æ–ª–ª–µ–≥–æ–π –Ω–∞ —Ç–µ–º—É –≤—ã–≥–æ—Ä–∞–Ω–∏—è. –ï—Å–ª–∏ –≤–∞–º –∫–∞–∂–µ—Ç—Å—è, —á—Ç–æ –≤—ã –Ω–∏ —Ä–∞–∑—É –Ω–µ —Å—Ç–∞–ª–∫–∏–≤–∞–ª–∏—Å—å —Å —ç—Ç–∏–º, —Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –≤—ã –ø—Ä–æ—Å—Ç–æ –Ω–µ –¥–æ—Ö–æ–¥–∏–ª–∏ –¥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–¥–∏–π.\n",
    "\n",
    "–í—ã–≥–æ—Ä–∞–Ω–∏–µ —Å–≤–æ–π—Å—Ç–≤–µ–Ω–Ω–æ –ª—é–±–æ–π –ø—Ä–æ—Ñ–µ—Å—Å–∏–∏, –Ω–æ –æ—Å–æ–±–µ–Ω–Ω–æ —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è –≤ it —Å—Ñ–µ—Ä–µ. –ü—Ä–æ–±–ª–µ–º–∞ —Å–µ—Ä—å–µ–∑–Ω–∞—è –∏ —Ä–∞–∑–±–∏—Ä–∞—Ç—å –∫–∞–∂–¥—ã–π –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–ª—É—á–∞–π –¥–æ–ª–∂–Ω—ã —Å–ø–µ—Ü–∏–∞–ª–∏—Å—Ç—ã, –Ω–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å, —á—Ç–æ —ç—Ç–æ –∏ –∫–∞–∫ –Ω–µ –¥–æ–≤–µ—Å—Ç–∏ —Å–µ–±—è –¥–æ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö —Å—Ç–∞–¥–∏–π, –Ω—É–∂–Ω–æ –∫–∞–∂–¥–æ–º—É. –ö–∞–∫-—Ç–æ —Ä–∞–∑ —è —Å–º–æ—Ç—Ä–µ–ª –æ—á–µ–Ω—å –ø–æ–ª–µ–∑–Ω—É—é –ª–µ–∫—Ü–∏—é –Ω–∞ —ç—Ç—É —Ç–µ–º—É, –ø–æ–¥–µ–ª—é—Å—å —Ç–µ–º, —á—Ç–æ –∑–∞–ø–æ–º–Ω–∏–ª –∏ —á—Ç–æ –ø—Ä–∏–º–µ–Ω—è—é —Å–∞–º.\n",
    "\n",
    "–í–∞–º –∫–æ–≥–¥–∞-–Ω–∏–±—É–¥—å –∫–∞–∑–∞–ª–æ—Å—å, —á—Ç–æ –≤—ã –Ω–µ –ø–æ–ª—É—á–∞–µ—Ç–µ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ –æ—Ç —Ä–∞–±–æ—Ç—ã? –ß—Ç–æ –Ω–µ —Ö–æ—á–µ—Ç—Å—è –¥–µ–ª–∞—Ç—å —Ä–∞–±–æ—á–∏–µ –∑–∞–¥–∞—á–∏ –∏ –¥–∞–∂–µ —Ç–æ, —á—Ç–æ —Ä–∞–Ω—å—à–µ –≤ —Ä–∞–±–æ—Ç–µ –≤–∞—Å —Ä–∞–¥–æ–≤–∞–ª–æ, –∫–∞–∂–µ—Ç—Å—è –∫–∞–∫–∏–º-—Ç–æ —Å–∫—É—á–Ω—ã–º –∏ –¥–∞–∂–µ –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã–º? –ê —Ü–µ–ª–∏ —É –≤–∞—Å –ø—Ä–æ—Å—Ç–æ –Ω–µ—Ç –∏ –≤—ã –∂–∏–≤—ë—Ç–µ –≤ –¥–Ω–µ —Å—É—Ä–∫–∞, –≥–¥–µ –Ω–µ—Ç –Ω–∏ –µ–¥–∏–Ω–æ–≥–æ –ø—Ä–æ—Å–≤–µ—Ç–∞? –¢–∞–∫ –ø—Ä–æ—è–≤–ª—è—é—Ç—Å—è –Ω–∞—á–∞–ª—å–Ω—ã–µ —Å—Ç–∞–¥–∏–∏ –≤—ã–≥–æ—Ä–∞–Ω–∏—è. –ï—Å–ª–∏ –Ω–µ –æ–±—Ä–∞—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ –¥–æ–ª–∂–Ω–æ–≥–æ –≤–Ω–∏–º–∞–Ω–∏—è, —Ç–æ –¥–∞–ª—å—à–µ –≤—Å–µ —Ç–æ–ª—å–∫–æ —É—Å—É–≥—É–±–∏—Ç—Å—è.\n",
    "\n",
    "–°–∞–º–∞ –ø—Ä–æ–±–ª–µ–º–∞ –∑–∞–∫–ª—é—á–∞–µ—Ç—Å—è –≤ –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ–º —É–≥–∞—Å–∞–Ω–∏–∏ ¬´—ç–Ω–µ—Ä–≥–∏–∏¬ª, –∫–æ—Ç–æ—Ä–æ–π –≤ –∫–æ–Ω–µ—á–Ω–æ–º –∏—Ç–æ–≥–µ —Ö–≤–∞—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ –Ω–∞ –ª–µ–∂–∞–Ω–∏–µ –Ω–∞ –¥–∏–≤–∞–Ω–µ. –ù–∞ —Å–∞–º–æ–º –¥–µ–ª–µ –µ—Å—Ç—å –Ω–∞—É—á–Ω–æ–µ –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ, —Å–≤—è–∑–∞–Ω–Ω–æ–µ —Å –≤—ã—Ä–∞–±–æ—Ç–∫–æ–π –Ω–µ–π—Ä–æ–º–µ–¥–∏–∞—Ç–æ—Ä–æ–≤ - –¥–æ—Ñ–∞–º–∏–Ω–∞, —Å–µ—Ä–æ—Ç–æ–Ω–∏–Ω–∞ –∏ –ø—Ä–æ—á–∏—Ö. –õ—É—á—à–µ –∑–∞–≥—É–≥–ª–∏—Ç—å, –Ω–æ —è —Ä–∞—Å—Å–∫–∞–∂—É –≤–∫—Ä–∞—Ç—Ü–µ. –î–æ—Ñ–∞–º–∏–Ω –æ—Ç–≤–µ—á–∞–µ—Ç –∑–∞ –≤–∞—à—É –º–æ—Ç–∏–≤–∞—Ü–∏—é –∏ —Å–∞–º–æ–µ –≥–ª–∞–≤–Ω–æ–µ - —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏–µ –æ—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω–Ω–æ–π –∑–∞–¥–∞—á–∏. –ü–æ–ª—É—á–∞–µ—Ç—Å—è —Ç–∞–∫–æ–π ¬´–Ω–∞—Ä–∫–æ—Ç–∏–∫¬ª - –≤—ã —Ö–æ—Ç–∏—Ç–µ —á—Ç–æ-—Ç–æ –∫—Ä—É—Ç–æ–µ —Å–¥–µ–ª–∞—Ç—å, —á—Ç–æ–±—ã –∏—Å–ø—ã—Ç–∞—Ç—å —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä–µ–Ω–∏–µ. –ï—Å–ª–∏ —É—Ä–æ–≤–Ω–∏ –Ω–µ–π—Ä–æ–º–µ–¥–∏–∞—Ç–æ—Ä–æ–≤ —Å–Ω–∏–∂–∞—é—Ç—Å—è, —á–µ–ª–æ–≤–µ–∫ –≤–ø–∞–¥–∞–µ—Ç –≤ –¥–µ–ø—Ä–µ—Å—Å–∏—é.\n",
    "\n",
    "–†–∞–∑–¥–µ–ª—è—é—Ç 4 —Å—Ç–∞–¥–∏–∏ –≤—ã–≥–æ—Ä–∞–Ω–∏—è. –ù–∞ –ø–µ—Ä–≤–æ–π –∏ –≤—Ç–æ—Ä–æ–π –≤—ã–≥–æ—Ä–∞–Ω–∏–µ —Å–æ–≤—Å–µ–º –Ω–µ–∑–∞–º–µ—Ç–Ω–æ, –µ–≥–æ –ª–µ–≥–∫–æ —Å–ø—É—Ç–∞—Ç—å —Å —É—Å—Ç–∞–ª–æ—Å—Ç—å—é. –¢—Ä–µ—Ç—å—è —Å—Ç–∞–¥–∏—è —É–∂–µ –≤—ã–¥–µ–ª—è–µ—Ç—Å—è –∏ –≤—ã –º–æ–∂–µ—Ç–µ –ø–æ–Ω—è—Ç—å - –¥–∞, —è –≤—ã–≥–æ—Ä–µ–ª, –º–Ω–µ –Ω–µ –Ω—Ä–∞–≤–∏—Ç—Å—è –º–æ—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç—å. –ê –Ω–∞ —á–µ—Ç–≤—ë—Ä—Ç–æ–π –Ω–µ —Ö–æ—á–µ—Ç—Å—è –¥–µ–ª–∞—Ç—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–∏—á–µ–≥–æ, –ø–æ–ª–Ω–∞—è –∞–ø–∞—Ç–∏—è –∏ –æ—Ç–≤—Ä–∞—â–µ–Ω–∏–µ –∫–æ –≤—Å–µ–º—É. –•–æ—Ä–æ—à–∞—è –Ω–æ–≤–æ—Å—Ç—å –≤ —Ç–æ–º, —á—Ç–æ –∏–∑ –ø–µ—Ä–≤—ã—Ö —Ç—Ä—ë—Ö —Å—Ç–∞–¥–∏–π –º–æ–∂–Ω–æ –≤—ã–π—Ç–∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ.\n",
    "\n",
    "–ê –∫–∞–∫ –≤—ã–π—Ç–∏ –∏–∑ —Ç–∞–∫–æ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è, —Ä–∞—Å—Å–∫–∞–∂–µ–º –≤ —Å–ª–µ–¥—É—é—â–µ–º –ø–æ—Å—Ç–µ üòâ\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "–ë–ª–∏–∑–∫–∏–π –¥—Ä—É–≥ –∏ —Å–æ—Ä–∞—Ç–Ω–∏–∫ –≠–¥–≤–∞—Ä–¥–∞ ‚Äî –ê–º–∏—Ä–∞–Ω –°–∞—Ä–¥–∞—Ä–æ–≤. –¢–æ–∂–µ –±–ª–æ–≥–µ—Ä. –ò –≤–æ—Ç —ç—Ç–æ—Ç –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å –°–æ–±—è–Ω–∏–Ω—ã–º (–¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª), –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏ —Å –ü—É—Ç–∏–Ω—ã–º. –≠—Ç–∞ –ø–∞—Ä–æ—á–∫–∞ –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –≠–¥–≤–∞—Ä–¥ –ë–∏–ª, –¥–∞–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ –±–ª–æ–≥–µ—Ä—Å–∫–æ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–µ. –ò–º —Å —Ä—É–∫ —Å—Ö–æ–¥–∏—Ç –±—É–∫–≤–∞–ª—å–Ω–æ –≤—Å–µ. –ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª, –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–π. –ö–æ—Ä—Ä—É–ø—Ü–∏—è –∏ –∫—É–º–æ–≤—Å—Ç–≤–æ. –ö—É–º–æ–≤—Å—Ç–≤–æ –∏ –∫–æ—Ä—Ä—É–ø—Ü–∏—è.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.morph = None\n",
    "        self.syntax = None\n",
    "    \n",
    "    def set_morph(self, morph):\n",
    "        self.morph = morph\n",
    "        \n",
    "    def set_syntax(self, syntax):\n",
    "        self.syntax = syntax\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.token\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.tokens = []\n",
    "        self.clean_tokens = []\n",
    "        self.syntax_tree = [] \n",
    "\n",
    "        self.tokenize()\n",
    "        self.clean()\n",
    "        \n",
    "    @property\n",
    "    def words(self):\n",
    "        return list(map(str, self.tokens))\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.tokens = [Token(_.text) for _ in tokenize(self.sentence)]\n",
    "        \n",
    "    def clean(self):\n",
    "        clean_text = re.sub('[^–∞-—è–ê-–Øa-zA-Z]', ' ', self.sentence)\n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "        self.clean_tokens = [Token(_.text) for _ in tokenize(clean_text)]\n",
    "        \n",
    "    def morph(self):\n",
    "        markup = morph(self.words)\n",
    "\n",
    "        for i, token in enumerate(markup.tokens):\n",
    "            self.tokens[i].set_morph(token)\n",
    "            \n",
    "    def syntax(self):\n",
    "        markup = syntax(self.words)\n",
    "\n",
    "        for i, token in enumerate(markup.tokens):\n",
    "            self.tokens[i].set_syntax(token)\n",
    "\n",
    "            source = int(token.head_id) - 1\n",
    "            target = int(token.id) - 1\n",
    "\n",
    "            if source > 0 and source != target:  # skip root, loops\n",
    "                self.syntax_tree.append([source, target, token.rel])\n",
    "                \n",
    "    def show_syntax(self):\n",
    "        if self.syntax_tree == []:\n",
    "            self.syntax()\n",
    "\n",
    "        show_dep_markup(self.words, self.syntax_tree)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.sentence\n",
    "        \n",
    "    def __repr__(self):\n",
    "        s = ''\n",
    "        for tok in self.tokens:\n",
    "            s += '[{}] '.format(tok)\n",
    "        return s\n",
    "\n",
    "\n",
    "class Text:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentences = []\n",
    "        \n",
    "        self.sentenize()\n",
    "        \n",
    "    def sentenize(self):\n",
    "        self.sentences = [Sentence(_.text) for _ in sentenize(self.text)]\n",
    "        \n",
    "    def morph_all_sentences(self):\n",
    "        for sent in self.sentences:\n",
    "            sent.morph()\n",
    "            \n",
    "    def syntax_all_sentences(self):\n",
    "        for sent in self.sentences:\n",
    "            sent.syntax()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n Sentence: '.join([str(sent) for sent in self.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text(source_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∏ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–π —Ä–∞–∑–±–æ—Ä "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.morph_all_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.syntax_all_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = text.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚îå‚ñ∫ –ë–ª–∏–∑–∫–∏–π  amod\n",
      "  ‚îå‚îÄ‚îî‚îÄ –¥—Ä—É–≥     \n",
      "  ‚îÇ ‚îå‚ñ∫ –∏        cc\n",
      "‚îå‚îÄ‚îî‚ñ∫‚îî‚îÄ —Å–æ—Ä–∞—Ç–Ω–∏–∫ conj\n",
      "‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫ –≠–¥–≤–∞—Ä–¥–∞  nmod\n",
      "‚îÇ   ‚îå‚ñ∫ ‚Äî        punct\n",
      "‚îî‚ñ∫‚îå‚îÄ‚îî‚îÄ –ê–º–∏—Ä–∞–Ω   appos\n",
      "‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫ –°–∞—Ä–¥–∞—Ä–æ–≤ flat:name\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .        punct\n"
     ]
    }
   ],
   "source": [
    "sentence.show_syntax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∏–º–µ–Ω–æ–≤–∞–Ω–Ω—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = ner(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\">\n",
       "–ë–ª–∏–∑–∫–∏–π –¥—Ä—É–≥ –∏ —Å–æ—Ä–∞—Ç–Ω–∏–∫ <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–≠–¥–≤–∞—Ä–¥–∞<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span> ‚Äî <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–ê–º–∏—Ä–∞–Ω –°–∞—Ä–¥–∞—Ä–æ–≤<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span>. –¢–æ–∂–µ –±–ª–æ–≥–µ—Ä. –ò –≤–æ—Ç —ç—Ç–æ—Ç <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–°–∞—Ä–¥–∞—Ä–æ–≤<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span> ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–°–æ–±—è–Ω–∏–Ω—ã–º<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span> (–¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª), –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏ —Å <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–ü—É—Ç–∏–Ω—ã–º<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span>. –≠—Ç–∞ –ø–∞—Ä–æ—á–∫–∞ <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–°–∞—Ä–¥–∞—Ä–æ–≤<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span> ‚Äî <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">–≠–¥–≤–∞—Ä–¥ –ë–∏–ª<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">PER</span></span>, –¥–∞–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ –±–ª–æ–≥–µ—Ä—Å–∫–æ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–µ. –ò–º —Å —Ä—É–∫ —Å—Ö–æ–¥–∏—Ç –±—É–∫–≤–∞–ª—å–Ω–æ –≤—Å–µ. –ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª, –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–π. –ö–æ—Ä—Ä—É–ø—Ü–∏—è –∏ –∫—É–º–æ–≤—Å—Ç–≤–æ. –ö—É–º–æ–≤—Å—Ç–≤–æ –∏ –∫–æ—Ä—Ä—É–ø—Ü–∏—è.\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_span_markup(markup.text, markup.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–∏—Å–∫ —Å–ª–æ–≤–æ—Å–æ—á–µ—Ç–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matches(rule, *lines):\n",
    "    parser = Parser(rule)\n",
    "    for line in lines:\n",
    "        matches = parser.findall(line)\n",
    "        matches = sorted(matches, key=lambda _: _.span)\n",
    "        spans = [_.span for _ in matches]\n",
    "        show_markup(line, spans)\n",
    "        if matches:\n",
    "            facts = [_.fact for _ in matches]\n",
    "            if len(facts) == 1:\n",
    "                facts = facts[0]\n",
    "            display(facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_gram(tag):\n",
    "    def check(v):\n",
    "        parse = pymorphy.parse(v)[0]\n",
    "        if not parse:\n",
    "            return False\n",
    "\n",
    "        return parse.tag.POS == tag\n",
    "    \n",
    "    return custom(check)\n",
    "\n",
    "INT = type('INT')\n",
    "NOUN = gram('NOUN')\n",
    "ADJF = gram('ADJF')\n",
    "ABBR = gram('Abbr')\n",
    "VERB = gram('VERB')\n",
    "PREP = gram('PREP')\n",
    "\n",
    "EQ_NOUN = exact_gram('NOUN')\n",
    "\n",
    "NUMBER = rule(\n",
    "    INT.optional(),\n",
    "    in_('.,').optional(),\n",
    "    INT\n",
    ")\n",
    "\n",
    "LAST = and_(\n",
    "    gram('Surn'),\n",
    "    not_(gram('Abbr')),\n",
    ")\n",
    "FIRST = and_(\n",
    "    gram('Name'),\n",
    "    not_(gram('Abbr')),\n",
    ")\n",
    "\n",
    "gnc = gnc_relation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "NounPhrase = fact('NounPhrase', ['adj', 'noun'])\n",
    "\n",
    "NumberPhrase = fact('NumberPhrase', ['number', 'noun'])\n",
    "\n",
    "Fact = fact('Fact', ['value'])\n",
    "\n",
    "Name = fact(\n",
    "    'Name',\n",
    "    ['first', 'last'],\n",
    ")\n",
    "\n",
    "NounPhraseRule = rule(\n",
    "    ADJF.optional().repeatable().interpretation(\n",
    "        NounPhrase.adj\n",
    "    ).match(gnc),\n",
    "\n",
    "    EQ_NOUN.repeatable().interpretation(\n",
    "        NounPhrase.noun\n",
    "    ).match(gnc)\n",
    ").interpretation(NounPhrase).interpretation(Fact.value)\n",
    "\n",
    "NumberPhraseRule = rule(\n",
    "    NUMBER.interpretation(\n",
    "        NumberPhrase.number\n",
    "    ),\n",
    "    EQ_NOUN.interpretation(\n",
    "        NumberPhrase.noun\n",
    "    )\n",
    ").interpretation(NumberPhrase).interpretation(Fact.value)\n",
    "\n",
    "NameRule = rule(\n",
    "    FIRST.interpretation(\n",
    "        Name.first\n",
    "    ).match(gnc),\n",
    "    LAST.interpretation(\n",
    "        Name.last\n",
    "    ).match(gnc)\n",
    ").interpretation(\n",
    "    Name\n",
    ")\n",
    "\n",
    "AllRules = rule(or_(\n",
    "    NumberPhraseRule,\n",
    "    NounPhraseRule\n",
    ")).interpretation(Fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª, –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ \n",
      "        ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ    ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ                                            \n",
      "–æ—á–µ–≤–∏–¥–Ω—ã–π.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Fact(\n",
       "     value=NounPhrase(\n",
       "         adj=None,\n",
       "         noun='–æ—Ç–≤–µ—Ç'\n",
       "     )\n",
       " ),\n",
       " Fact(\n",
       "     value=NounPhrase(\n",
       "         adj=None,\n",
       "         noun='–≤–æ–ø—Ä–æ—Å'\n",
       "     )\n",
       " )]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_matches(AllRules, text.sentences[5].sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ—Ä–æ—Ç–∫–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ –¥–µ—Ä–µ–≤—É"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_head(sentence, token):\n",
    "    return next((t for t in sentence.tokens if t.syntax.id == token.syntax.head_id), None)\n",
    "\n",
    "def find_token_children(sentence, token):\n",
    "    return list(filter(lambda t: t.syntax.head_id == token.syntax.id, sentence.tokens))\n",
    "\n",
    "def distance_from_root(sentence, token, was_here=[]):\n",
    "    if token.syntax.rel in ['root', 'ccomp', 'xcomp', 'acl', 'acl:relcl', 'parataxis', 'advcl'] or not token or token in was_here:\n",
    "        return 0\n",
    "    \n",
    "    append = 1\n",
    "    \n",
    "    if token.syntax.rel in ['conj', 'obj', '' 'cc']:\n",
    "        append = 0\n",
    "    \n",
    "    return append + distance_from_root(sentence, find_token_head(sentence, token), was_here + [token])\n",
    "\n",
    "def shorten(sentence, percent=0.5, show_syntax=True):\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    required_filter = lambda t: t.syntax.rel in ['nsubj', 'obj', 'root']\n",
    "    \n",
    "    nummod_filter = lambda t: t.syntax.rel.startswith('nummod')\n",
    "    advmod_filter = lambda t: t.syntax.rel.startswith('advmod')\n",
    "    case_filter = lambda t: t.syntax.rel == 'case'\n",
    "    empty_modifier_filter = lambda t: t.syntax.rel in ['nmod', 'amod'] and len(find_token_children(sentence, t)) == 0\n",
    "    discourse_filter = lambda t: t.syntax.rel in ['discourse']\n",
    "    \n",
    "    if show_syntax:\n",
    "        sentence.show_syntax()\n",
    "\n",
    "    root = next((t for t in sentence.tokens if t.syntax.rel == 'root'), None)\n",
    "    \n",
    "    if not root:\n",
    "        return\n",
    "    \n",
    "    tokens_distance = [(t, distance_from_root(sentence, t)) for t in sentence.tokens]\n",
    "    max_depth = max(t[1] for t in tokens_distance)\n",
    "    max_allowed_token_depth = max_depth * percent\n",
    "\n",
    "    was_changed = True\n",
    "    \n",
    "    # TODO —Ç—É—Ç –Ω–∞–≤–µ—Ä–Ω–æ —Å—Ç–æ–∏—Ç –≤—Å–µ-—Ç–∞–∫–∏ –ø—Ä–æ—Ö–æ–¥–∏—Ç—å—Å—è –æ—Ç –∫–æ—Ä–Ω–µ–π, —á—Ç–æ–±—ã –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É—á–∏—Ç—ã–≤–∞—Ç—å, –∫–∞–∫–∏–µ —Ç–æ–∫–µ–Ω—ã –≤–∫–ª—é—á–µ–Ω—ã, –∫–∞–∫–∏–µ –Ω–µ—Ç.\n",
    "    # –ò–Ω–∞—á–µ –∏–∑-–∑–∞ –ø–æ—Ä—è–¥–∫–∞ –æ–±—Ö–æ–¥–∞ —Ç–µ—Ä—è–µ—Ç—Å—è —á–∞—Å—Ç—å —Ç–æ–∫–µ–Ω–æ–≤ (—Ä–æ–¥–∏—Ç–µ–ª—å –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –ø–æ–∑–∂–µ —Ä–µ–±–µ–Ω–∫–∞ –∏ –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è —Ä–µ–±–µ–Ω–∫–∞ –Ω–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç)\n",
    "    # –£–∂–µ –ø–µ—Ä–µ–¥–µ–ª–∞–ª –Ω–∞ while, —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞—Ç—å –ª–æ–≥–∏–∫—É –æ–±—Ö–æ–¥–∞. –ú–æ–∂–Ω–æ –ø–æ—Ç–æ–º –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å –±–æ–ª–µ–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ\n",
    "    \n",
    "    # TODO –µ—â–µ —Å—Ç–æ–∏—Ç —Ä–∞–∑–¥–µ–ª—è—Ç—å –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ —Å–æ—Å—Ç–∞–≤–Ω—ã–µ —á–∞—Å—Ç–∏ (—Å–ª–æ–∂–Ω–æ—Å–æ—á–∏–Ω–µ–Ω–Ω—ã–µ/–ø–æ–¥—á–∏–Ω–µ–Ω–Ω—ã–µ),\n",
    "    # —á—Ç–æ–±—ã —É—á–∏—Ç—ã–≤–∞—Ç—å –º–∞–∫—Å. –¥–∏—Å—Ç–∞–Ω—Ü–∏—é –æ—Ç –∫–æ—Ä–Ω—è –≤ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –≤–µ—Ç–∫–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "\n",
    "    i = -1\n",
    "    while was_changed:\n",
    "        i += 1\n",
    "        was_changed = False\n",
    "\n",
    "        def push_to_result(t):\n",
    "            nonlocal was_changed\n",
    "            if t.syntax.id not in map(lambda t: t.syntax.id, filtered_tokens):\n",
    "                filtered_tokens.append(t)\n",
    "                was_changed = True\n",
    "\n",
    "        for t, d in tokens_distance:\n",
    "            t_head = find_token_head(sentence, t)\n",
    "            t_head_included = t_head and t_head.syntax.id in map(lambda t: t.syntax.id, filtered_tokens)\n",
    "\n",
    "            # –±–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ top-N —Å–ª–æ–≤ –æ—Ç –∫–æ—Ä–Ω—è\n",
    "            # –∏—Å–∫–ª—é—á–∞–µ–º \"–≤–∏—Å—è—á–∏–µ\" –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã, –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–∫–ª—é—á–µ–Ω—ã –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∞–≤–∏–ª–∞–º –Ω–∏–∂–µ\n",
    "            if (d <= max_allowed_token_depth or required_filter(t)) and \\\n",
    "                not empty_modifier_filter(t) and \\\n",
    "                not discourse_filter(t):\n",
    "                push_to_result(t)\n",
    "\n",
    "            if t_head_included and t_head:\n",
    "                # –±–µ—Ä–µ–º –≤—Å–µ —á–∏—Å–ª–∞, —á—Ç–æ–±—ã –Ω–µ –ø–æ—Ç–µ—Ä—è—Ç—å —Å–º—ã—Å–ª\n",
    "                if nummod_filter(t):\n",
    "                    push_to_result(t)\n",
    "                \n",
    "                # –Ω–∞ <- IPO, nmod -> case. –ß—Ç–æ–±—ã –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –æ—Å—Ç–∞–≤–∞–ª–æ—Å—å —á–∏—Ç–∞–µ–º—ã–º, –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–µ–¥–ª–æ–≥–∏, —á—Ç–æ–±—ã –Ω–µ –±—ã–ª–æ –≤–∏—Å—è—á–∏—Ö —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö\n",
    "                if case_filter(t):\n",
    "                    push_to_result(t)\n",
    "\n",
    "                # –≤—ã—Ö–æ–¥–∞ -> –∫–æ–º–ø–∞–Ω–∏–∏, obl -> any\n",
    "                if t_head.syntax.rel == 'obl':\n",
    "                    push_to_result(t)\n",
    "\n",
    "                # –Ω–µ <- –¥–æ—Ä–æ–∂–µ, advmod -> advmod\n",
    "                if t_head.syntax.rel == t.syntax.rel and t.syntax.rel in ['advmod', 'amod']:\n",
    "                    push_to_result(t)\n",
    "\n",
    "                # –±–µ—Ä–µ–º —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ –º–æ–¥–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –∫ –æ–±—ä–µ–∫—Ç–∞–º –∏ —Å—É–±—ä–µ–∫—Ç–∞–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è\n",
    "                if t_head.syntax.rel in ['nsubj', 'obj'] and t.syntax.rel in ['nmod']:\n",
    "                    push_to_result(t)\n",
    "    \n",
    "    print(i + 1, 'iterations')\n",
    "    \n",
    "    filtered_tokens.sort(key=lambda t: int(t.syntax.id))\n",
    "\n",
    "    return list(map(lambda t: t.token, filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ‚îå‚ñ∫ –ë–ª–∏–∑–∫–∏–π  amod\n",
      "  ‚îå‚îÄ‚îî‚îÄ –¥—Ä—É–≥     \n",
      "  ‚îÇ ‚îå‚ñ∫ –∏        cc\n",
      "‚îå‚îÄ‚îî‚ñ∫‚îî‚îÄ —Å–æ—Ä–∞—Ç–Ω–∏–∫ conj\n",
      "‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫ –≠–¥–≤–∞—Ä–¥–∞  nmod\n",
      "‚îÇ   ‚îå‚ñ∫ ‚Äî        punct\n",
      "‚îî‚ñ∫‚îå‚îÄ‚îî‚îÄ –ê–º–∏—Ä–∞–Ω   appos\n",
      "‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫ –°–∞—Ä–¥–∞—Ä–æ–≤ flat:name\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .        punct\n",
      "  ‚îå‚ñ∫ –¢–æ–∂–µ   advmod\n",
      "‚îå‚îÄ‚îî‚îÄ –±–ª–æ–≥–µ—Ä \n",
      "‚îî‚îÄ‚îÄ‚ñ∫ .      punct\n",
      "2 iterations\n",
      "–¢–æ–∂–µ –±–ª–æ–≥–µ—Ä. 3\n",
      "–±–ª–æ–≥–µ—Ä 1\n",
      "-----------\n",
      "\n",
      "\n",
      "    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ –ò         cc\n",
      "    ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ –≤–æ—Ç       advmod\n",
      "    ‚îÇ ‚îÇ   ‚îå‚ñ∫ —ç—Ç–æ—Ç      det\n",
      "    ‚îÇ ‚îÇ ‚îå‚ñ∫‚îî‚îÄ –°–∞—Ä–¥–∞—Ä–æ–≤  nsubj\n",
      "    ‚îÇ ‚îÇ ‚îÇ ‚îå‚ñ∫ ‚Äî         punct\n",
      "‚îå‚îÄ‚îå‚îÄ‚îî‚îÄ‚îî‚îÄ‚îî‚îÄ‚îî‚îÄ –¥–µ–ª–∞–ª     \n",
      "‚îÇ ‚îÇ   ‚îå‚îÄ‚îî‚îÄ‚îÄ‚ñ∫ –∏–Ω—Ç–µ—Ä–≤—å—é  obj\n",
      "‚îÇ ‚îÇ   ‚îÇ   ‚îå‚ñ∫ —Å         case\n",
      "‚îÇ ‚îÇ ‚îå‚îÄ‚îî‚îÄ‚îÄ‚ñ∫‚îî‚îÄ –°–æ–±—è–Ω–∏–Ω—ã–º nmod\n",
      "‚îÇ ‚îÇ ‚îÇ   ‚îå‚îÄ‚îÄ‚ñ∫ (         punct\n",
      "‚îÇ ‚îÇ ‚îÇ   ‚îÇ ‚îå‚ñ∫ –¥–∏—Ñ–∏—Ä–∞–º–±—ã nsubj\n",
      "‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫‚îî‚îÄ‚îî‚îÄ –ø–µ–ª       parataxis\n",
      "‚îÇ ‚îÇ     ‚îî‚îÄ‚îÄ‚ñ∫ )         punct\n",
      "‚îÇ ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ,         punct\n",
      "‚îÇ ‚îÇ   ‚îÇ ‚îå‚ñ∫‚îå‚îÄ –∞         cc\n",
      "‚îÇ ‚îÇ   ‚îÇ ‚îÇ ‚îî‚ñ∫ —Ç–∞–∫–∂–µ     fixed\n",
      "‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫‚îî‚îÄ‚îî‚îÄ‚îå‚îÄ –∑–∞–¥–∞–≤–∞–ª   conj\n",
      "‚îÇ     ‚îå‚îÄ‚îÄ‚îÄ‚îî‚ñ∫ –≤–æ–ø—Ä–æ—Å—ã   obj\n",
      "‚îÇ     ‚îÇ ‚îå‚îÄ‚îÄ‚ñ∫ –Ω–∞        case\n",
      "‚îÇ     ‚îÇ ‚îÇ ‚îå‚ñ∫ –ø—Ä—è–º–æ–π    amod\n",
      "‚îÇ     ‚îî‚ñ∫‚îî‚îÄ‚îî‚îÄ –ª–∏–Ω–∏–∏     nmod\n",
      "‚îÇ     ‚îÇ   ‚îå‚ñ∫ —Å         case\n",
      "‚îÇ     ‚îî‚îÄ‚îÄ‚ñ∫‚îî‚îÄ –ü—É—Ç–∏–Ω—ã–º   nmod\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .         punct\n",
      "3 iterations\n",
      "–ò –≤–æ—Ç —ç—Ç–æ—Ç –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å –°–æ–±—è–Ω–∏–Ω—ã–º (–¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª), –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏ —Å –ü—É—Ç–∏–Ω—ã–º. 24\n",
      "–ò –≤–æ—Ç –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å –°–æ–±—è–Ω–∏–Ω—ã–º ( –¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª ) , –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ª–∏–Ω–∏–∏ . 20\n",
      "-----------\n",
      "\n",
      "\n",
      "      ‚îå‚ñ∫ –≠—Ç–∞        det\n",
      "‚îå‚ñ∫‚îå‚îÄ‚îå‚îÄ‚îî‚îÄ –ø–∞—Ä–æ—á–∫–∞    nsubj\n",
      "‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫ –°–∞—Ä–¥–∞—Ä–æ–≤   nmod\n",
      "‚îÇ ‚îÇ   ‚îå‚ñ∫ ‚Äî          punct\n",
      "‚îÇ ‚îî‚ñ∫‚îå‚îÄ‚îî‚îÄ –≠–¥–≤–∞—Ä–¥     appos\n",
      "‚îÇ   ‚îî‚îÄ‚îÄ‚ñ∫ –ë–∏–ª        flat:name\n",
      "‚îÇ   ‚îå‚îÄ‚îÄ‚ñ∫ ,          punct\n",
      "‚îÇ   ‚îÇ ‚îå‚ñ∫ –¥–∞–≤–Ω–æ      advmod\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îî‚îÄ‚îî‚îÄ –≤—ã–∑—ã–≤–∞–µ—Ç   \n",
      "‚îÇ ‚îå‚îÄ‚îî‚îÄ‚îÄ‚ñ∫ –≤–æ–ø—Ä–æ—Å—ã    obj\n",
      "‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚ñ∫ –≤          case\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îå‚ñ∫ –±–ª–æ–≥–µ—Ä—Å–∫–æ–º amod\n",
      "‚îÇ ‚îî‚ñ∫‚îî‚îÄ‚îî‚îÄ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ nmod\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .          punct\n",
      "3 iterations\n",
      "–≠—Ç–∞ –ø–∞—Ä–æ—á–∫–∞ –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –≠–¥–≤–∞—Ä–¥ –ë–∏–ª, –¥–∞–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ –±–ª–æ–≥–µ—Ä—Å–∫–æ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–µ. 14\n",
      "–ø–∞—Ä–æ—á–∫–∞ –°–∞—Ä–¥–∞—Ä–æ–≤ , –¥–∞–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ —Å–æ–æ–±—â–µ—Å—Ç–≤–µ . 9\n",
      "-----------\n",
      "\n",
      "\n",
      "  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ –ò–º        iobj\n",
      "  ‚îÇ   ‚îå‚ñ∫ —Å         case\n",
      "  ‚îÇ ‚îå‚ñ∫‚îî‚îÄ —Ä—É–∫       obl\n",
      "‚îå‚îÄ‚îî‚îÄ‚îî‚îÄ‚îÄ‚îÄ —Å—Ö–æ–¥–∏—Ç    \n",
      "‚îÇ ‚îÇ   ‚îå‚ñ∫ –±—É–∫–≤–∞–ª—å–Ω–æ advmod\n",
      "‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫‚îî‚îÄ –≤—Å–µ       nsubj\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .         punct\n",
      "3 iterations\n",
      "–ò–º —Å —Ä—É–∫ —Å—Ö–æ–¥–∏—Ç –±—É–∫–≤–∞–ª—å–Ω–æ –≤—Å–µ. 7\n",
      "–ò–º —Å —Ä—É–∫ —Å—Ö–æ–¥–∏—Ç –≤—Å–µ . 6\n",
      "-----------\n",
      "\n",
      "\n",
      "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ –ü–æ—ç—Ç–æ–º—É       advmod\n",
      "‚îÇ ‚îå‚ñ∫‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îå‚îÄ‚îÄ‚îÄ –æ—Ç–≤–µ—Ç         nsubj\n",
      "‚îÇ ‚îÇ ‚îÇ     ‚îÇ ‚îå‚ñ∫ –Ω–∞            case\n",
      "‚îÇ ‚îÇ ‚îÇ     ‚îî‚ñ∫‚îî‚îÄ –≤–æ–ø—Ä–æ—Å        nmod\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ¬´             punct\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ –∞             cc\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚îÄ‚îÄ‚ñ∫ –∫–∞–∫           advmod\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îå‚ñ∫ —Ç–∞–∫           advmod\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îî‚îÄ‚îî‚îÄ –≤—ã—à–ª–æ         \n",
      "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚ñ∫ ?             punct\n",
      "‚îÇ ‚îÇ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ ¬ª             punct\n",
      "‚îÇ ‚îÇ ‚îî‚ñ∫‚îÇ        ,             punct\n",
      "‚îî‚îÄ‚îî‚îÄ‚îÄ‚îÄ‚îî‚îÄ‚îå‚îÄ‚îå‚îÄ‚îÄ‚îÄ –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è \n",
      "        ‚îÇ ‚îÇ ‚îå‚ñ∫ –¥–æ–≤–æ–ª—å–Ω–æ      advmod\n",
      "        ‚îÇ ‚îî‚ñ∫‚îî‚îÄ –æ—á–µ–≤–∏–¥–Ω—ã–π     nsubj\n",
      "        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ .             punct\n",
      "3 iterations\n",
      "–ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª, –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–π. 16\n",
      "–ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´ –∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ ? ¬ª –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –æ—á–µ–≤–∏–¥–Ω—ã–π . 14\n",
      "-----------\n",
      "\n",
      "\n",
      "   –ö–æ—Ä—Ä—É–ø—Ü–∏—è \n",
      "‚îå‚ñ∫ –∏         cc\n",
      "‚îî‚îÄ –∫—É–º–æ–≤—Å—Ç–≤–æ \n",
      "   .         \n",
      "2 iterations\n",
      "–ö–æ—Ä—Ä—É–ø—Ü–∏—è –∏ –∫—É–º–æ–≤—Å—Ç–≤–æ. 4\n",
      "–ö–æ—Ä—Ä—É–ø—Ü–∏—è –∏ –∫—É–º–æ–≤—Å—Ç–≤–æ 3\n",
      "-----------\n",
      "\n",
      "\n",
      "   –ö—É–º–æ–≤—Å—Ç–≤–æ \n",
      "‚îå‚ñ∫ –∏         cc\n",
      "‚îî‚îÄ –∫–æ—Ä—Ä—É–ø—Ü–∏—è \n",
      "   .         \n",
      "2 iterations\n",
      "–ö—É–º–æ–≤—Å—Ç–≤–æ –∏ –∫–æ—Ä—Ä—É–ø—Ü–∏—è. 4\n",
      "–ö—É–º–æ–≤—Å—Ç–≤–æ –∏ –∫–æ—Ä—Ä—É–ø—Ü–∏—è 3\n",
      "-----------\n",
      "\n",
      "\n",
      "src len 72 short 56 diff 16\n",
      "78%\n"
     ]
    }
   ],
   "source": [
    "total_src_len = 0\n",
    "total_short_len = 0\n",
    "\n",
    "for sent in text.sentences:\n",
    "    short = shorten(sent, 0.5, True)\n",
    "    \n",
    "    if not short:\n",
    "        continue\n",
    "    \n",
    "    src_len = len(sent.tokens)\n",
    "    short_len = len(short)\n",
    "    \n",
    "    print(sent, src_len)\n",
    "    print(' '.join(short), short_len)\n",
    "    print('-----------\\n\\n')\n",
    "    \n",
    "    total_short_len += short_len\n",
    "    total_src_len += src_len\n",
    "    \n",
    "print('src len', total_src_len, 'short', total_short_len, 'diff', total_src_len - total_short_len)\n",
    "print('{:.0f}%'.format(total_short_len / total_src_len * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text = model(source_txt, num_sentences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ë–ª–∏–∑–∫–∏–π –¥—Ä—É–≥ –∏ —Å–æ—Ä–∞—Ç–Ω–∏–∫ –≠–¥–≤–∞—Ä–¥–∞ ‚Äî –ê–º–∏—Ä–∞–Ω –°–∞—Ä–¥–∞—Ä–æ–≤. –¢–æ–∂–µ –±–ª–æ–≥–µ—Ä. –ò –≤–æ—Ç —ç—Ç–æ—Ç –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å –°–æ–±—è–Ω–∏–Ω—ã–º (–¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª), –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏ —Å –ü—É—Ç–∏–Ω—ã–º. –≠—Ç–∞ –ø–∞—Ä–æ—á–∫–∞ –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –≠–¥–≤–∞—Ä–¥ –ë–∏–ª, –¥–∞–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ –±–ª–æ–≥–µ—Ä—Å–∫–æ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–µ. –ò–º —Å —Ä—É–∫ —Å—Ö–æ–¥–∏—Ç –±—É–∫–≤–∞–ª—å–Ω–æ –≤—Å–µ. –ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª, –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–π. –ö–æ—Ä—Ä—É–ø—Ü–∏—è –∏ –∫—É–º–æ–≤—Å—Ç–≤–æ. –ö—É–º–æ–≤—Å—Ç–≤–æ –∏ –∫–æ—Ä—Ä—É–ø—Ü–∏—è.\n",
      "\n",
      "-----\n",
      "–ë–ª–∏–∑–∫–∏–π –¥—Ä—É–≥ –∏ —Å–æ—Ä–∞—Ç–Ω–∏–∫ –≠–¥–≤–∞—Ä–¥–∞ ‚Äî –ê–º–∏—Ä–∞–Ω –°–∞—Ä–¥–∞—Ä–æ–≤. –ò –≤–æ—Ç —ç—Ç–æ—Ç –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å –°–æ–±—è–Ω–∏–Ω—ã–º (–¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª), –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏ —Å –ü—É—Ç–∏–Ω—ã–º. –ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª,\n"
     ]
    }
   ],
   "source": [
    "print(source_txt)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(short_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## –ß–∞—Å—Ç–æ—Ç–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "def freq_summarize(source_text, num_sentences=3):\n",
    "    text = Text(source_text)\n",
    "    \n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for sentence in text.sentences:\n",
    "        for word in sentence.clean_tokens:\n",
    "            if word.token not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word.token] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.token] += 1\n",
    "    \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    sentence_scores = {}\n",
    "    for sentence in text.sentences:\n",
    "        for word in sentence.clean_tokens:\n",
    "            if word.token in word_frequencies.keys():\n",
    "                if sentence.sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence.sentence] = word_frequencies[word.token]\n",
    "                else:\n",
    "                    sentence_scores[sentence.sentence] += word_frequencies[word.token]\n",
    "                    \n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    \n",
    "    summary = ''\n",
    "    for sentence in text.sentences:\n",
    "        if sentence.sentence in summary_sentences:\n",
    "            summary += sentence.sentence + ' '\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ò –≤–æ—Ç —ç—Ç–æ—Ç –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –¥–µ–ª–∞–ª –∏–Ω—Ç–µ—Ä–≤—å—é —Å –°–æ–±—è–Ω–∏–Ω—ã–º (–¥–∏—Ñ–∏—Ä–∞–º–±—ã –ø–µ–ª), –∞ —Ç–∞–∫–∂–µ –∑–∞–¥–∞–≤–∞–ª –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ø—Ä—è–º–æ–π –ª–∏–Ω–∏–∏ —Å –ü—É—Ç–∏–Ω—ã–º. –≠—Ç–∞ –ø–∞—Ä–æ—á–∫–∞ –°–∞—Ä–¥–∞—Ä–æ–≤ ‚Äî –≠–¥–≤–∞—Ä–¥ –ë–∏–ª, –¥–∞–≤–Ω–æ –≤—ã–∑—ã–≤–∞–µ—Ç –≤–æ–ø—Ä–æ—Å—ã –≤ –±–ª–æ–≥–µ—Ä—Å–∫–æ–º —Å–æ–æ–±—â–µ—Å—Ç–≤–µ. –ü–æ—ç—Ç–æ–º—É –æ—Ç–≤–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∞ –∫–∞–∫ —Ç–∞–∫ –≤—ã—à–ª–æ?¬ª, –Ω–∞–ø—Ä–∞—à–∏–≤–∞–µ—Ç—Å—è –¥–æ–≤–æ–ª—å–Ω–æ –æ—á–µ–≤–∏–¥–Ω—ã–π. \n"
     ]
    }
   ],
   "source": [
    "print(freq_summarize(source_txt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–∏—Å–∫ —É–ø–æ–º–∏–Ω–∞–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from russiannames.parser import NamesParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_name(name):\n",
    "    def get_noun(n):\n",
    "        name_forms = pymorphy.parse(n)\n",
    "        normal_form = list(filter(lambda x: 'Name' in x.tag or 'Surn' in x.tag, name_forms))\n",
    "        \n",
    "        normal_form = normal_form[0].normal_form if len(normal_form) > 0 else n\n",
    "        \n",
    "        return normal_form if len(normal_form) <= len(n) else n\n",
    "\n",
    "    norm_name = ''\n",
    "    for name_part in name.split(' '):\n",
    "        norm_name += get_noun(name_part) + ' '\n",
    "        \n",
    "    return norm_name[:-1].lower()\n",
    "\n",
    "def count_persons(text_str):\n",
    "    parser = NamesParser()\n",
    "    markup = ner(text_str)\n",
    "    \n",
    "    person_spans = list(filter(lambda x: x.type == 'PER', markup.spans))\n",
    "\n",
    "    names = {}\n",
    "\n",
    "    def put_to_names(name):\n",
    "        if name in names:\n",
    "            names[name] += 1\n",
    "            return True\n",
    "\n",
    "        if name[:-1] in names:\n",
    "            names[name[:-1]] += 1\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    for span in person_spans:\n",
    "        put = False\n",
    "\n",
    "        span_text = text_str[span.start:span.stop]\n",
    "        name = parser.parse(span_text)\n",
    "\n",
    "        if not name['text']:\n",
    "            continue\n",
    "\n",
    "        name = normalize_name(name['text'])\n",
    "\n",
    "        name_parts = name.split(' ')        \n",
    "\n",
    "        if len(name_parts) == 1:\n",
    "            if put_to_names(name):\n",
    "                continue\n",
    "\n",
    "        names_keys = names.keys()\n",
    "\n",
    "        for names_key in names_keys:\n",
    "            if len(list(set(name_parts) & set(names_key.split(' ')))) == len(name_parts):\n",
    "                if put_to_names(names_key):\n",
    "                    put = True\n",
    "                    break\n",
    "\n",
    "        if not put:\n",
    "            names[name] = 1\n",
    "            \n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'—ç–¥–≤–∞—Ä–¥': 1, '–∞–º–∏—Ä–∞–Ω —Å–∞—Ä–¥–∞—Ä–æ–≤': 3, '—Å–æ–±—è–Ω–∏–Ω': 1, '–ø—É—Ç–∏–Ω': 1, '—ç–¥–≤–∞—Ä–¥ –±–∏–ª': 1}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_persons(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_persons(text, *persons_to_check):\n",
    "    names = count_persons(text)\n",
    "    identified = {}\n",
    "\n",
    "    for p in persons_to_check:\n",
    "        person = p.lower()\n",
    "        if names.get(person):\n",
    "            identified[p] = names[person]\n",
    "            \n",
    "    return identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "identify_persons(text.text, '–°–µ—Ä–≥–µ–π –ª–æ–º–∞–∫–∏–Ω')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –í—ã–¥–µ–ª–µ–Ω–∏–µ –∫–æ–Ω—Ç–∞–∫—Ç–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "emails = re.compile(\"[a-z0-9\\.\\-+_]+@[a-z0-9\\.\\-+_]+\\.[a-z]+\")\n",
    "urls = re.compile('https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+')\n",
    "phones = re.compile(\"((?:\\+\\d{2}[-\\.\\s]??|\\d{4}[-\\.\\s]??)?(?:\\d{3}[-\\.\\s]??\\d{3}[-\\.\\s]??\\d{4}|\\(\\d{3}\\)\\s*\\d{3}[-\\.\\s]??\\d{4}|\\d{3}[-\\.\\s]??\\d{4}))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "email: s.chernobrovkin@ktsstudio.ru\n",
    "phone: 8(911)123-1232\n",
    "url: https://kts.studio\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(911)123-1232']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phones.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s.chernobrovkin@ktsstudio.ru']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://kts.studio']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls.findall(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –°–µ–Ω—Ç–∏–º–µ–Ω—Ç-–∞–Ω–∞–ª–∏–∑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dostoevsky.tokenization import RegexTokenizer\n",
    "from dostoevsky.models import FastTextSocialNetworkModel\n",
    "\n",
    "import spacy\n",
    "from spacytextblob.spacytextblob import SpacyTextBlob\n",
    "\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RegexTokenizer()\n",
    "model = FastTextSocialNetworkModel(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_txt = \"\"\"\n",
    "‚ö°Ô∏è–ù–∞–≤–∞–ª—å–Ω—ã–π –æ–±—ä—è–≤–∏–ª –≥–æ–ª–æ–¥–æ–≤–∫—É.\n",
    "\n",
    "\"–ü–æ—á–µ–º—É –∑–∞–∫–ª—é—á—ë–Ω–Ω—ã–µ –æ–±—ä—è–≤–ª—è—é—Ç –≥–æ–ª–æ–¥–æ–≤–∫–∏? –ê –Ω–µ—Ç—É —É —Ç–µ–±—è –¥—Ä—É–≥–∏—Ö –º–µ—Ç–æ–¥–æ–≤ –±–æ—Ä—å–±—ã, –≤–æ—Ç –∏ –æ–±—ä—è–≤–ª—è–µ—à—å\", - –æ–±—ä—è—Å–Ω–∏–ª –ê–ª–µ–∫—Å–µ–π —É —Å–µ–±—è –≤ –ò–Ω—Å—Ç–∞–≥—Ä–∞–º–µ.\n",
    "\"\"\"\n",
    "\n",
    "comments = [\n",
    "    \"- 1 –≤—Ä–∞–≥ –Ω–∞—Ä–æ–¥–∞.  –ù–µ –∑—Ä—è –°—Ç–∞–ª–∏–Ω —Å—Ç—Ä–µ–ª—è–ª –∑–∞ –≥–æ–ª–æ–¥–æ–≤–∫—É –∏–ª–∏ —Å—Ä–æ–∫ –ø—Ä–∏–±–∞–≤–ª—è–ª.\",\n",
    "    \"–ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è –∏–º –±—É–¥–µ—Ç –ø–ª–µ–≤–∞—Ç—å\",\n",
    "    \"–í –ë–µ–ª–∞—Ä—É—Å–∏ –≥–æ–ª–æ–¥–∞—é—Ç –∏ –õ—É–∫–µ –Ω–∞—Å—Ä–∞—Ç—å)\",\n",
    "    \"–•–æ—Ç–µ–ª–æ—Å—å –±—ã —á—Ç–æ–±—ã –æ–±—â–µ—Å—Ç–≤–æ –æ—Å–æ–∑–Ω–∞–ª–æ, —á—Ç–æ –ù–∞–≤–∞–ª—å–Ω—ã–π —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø–æ–ª–∏—Ç –∑–∞–∫–ª—é—á—ë–Ω–Ω—ã–π. –ê –≤ –∫–∞–∫–æ–π-—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏ —Å–∏–º–≤–æ–ª –±–æ—Ä—å–±—ã —Å —Ä–µ–∂–∏–º–æ–º\"\n",
    "]\n",
    "\n",
    "results = model.predict(comments, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1 –≤—Ä–∞–≥ –Ω–∞—Ä–æ–¥–∞.  –ù–µ –∑—Ä—è –°—Ç–∞–ª–∏–Ω —Å—Ç—Ä–µ–ª—è–ª –∑–∞ –≥–æ–ª–æ–¥–æ–≤–∫—É –∏–ª–∏ —Å—Ä–æ–∫ –ø—Ä–∏–±–∞–≤–ª—è–ª. -> {'negative': 0.4765896201133728, 'skip': 0.4688006341457367}\n",
      "–ú–Ω–µ –∫–∞–∂–µ—Ç—Å—è –∏–º –±—É–¥–µ—Ç –ø–ª–µ–≤–∞—Ç—å -> {'negative': 0.4532718360424042, 'neutral': 0.3629792034626007}\n",
      "–í –ë–µ–ª–∞—Ä—É—Å–∏ –≥–æ–ª–æ–¥–∞—é—Ç –∏ –õ—É–∫–µ –Ω–∞—Å—Ä–∞—Ç—å) -> {'neutral': 0.348655104637146, 'skip': 0.20690405368804932}\n",
      "–•–æ—Ç–µ–ª–æ—Å—å –±—ã —á—Ç–æ–±—ã –æ–±—â–µ—Å—Ç–≤–æ –æ—Å–æ–∑–Ω–∞–ª–æ, —á—Ç–æ –ù–∞–≤–∞–ª—å–Ω—ã–π —ç—Ç–æ –ø—Ä–æ—Å—Ç–æ –ø–æ–ª–∏—Ç –∑–∞–∫–ª—é—á—ë–Ω–Ω—ã–π. –ê –≤ –∫–∞–∫–æ–π-—Ç–æ —Å—Ç–µ–ø–µ–Ω–∏ —Å–∏–º–≤–æ–ª –±–æ—Ä—å–±—ã —Å —Ä–µ–∂–∏–º–æ–º -> {'negative': 0.6370407938957214, 'neutral': 0.275139719247818}\n"
     ]
    }
   ],
   "source": [
    "for message, sentiment in zip(comments, results):\n",
    "    print(message, '->', sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# –ü–æ–ª—è—Ä–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polyglot.downloader import downloader\n",
    "from polyglot.text import Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text(source_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.polarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspirant",
   "language": "python",
   "name": "aspirant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
