{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load: ok\n"
     ]
    }
   ],
   "source": [
    "from navec import Navec\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_box_markup as show_span_markup, show_ascii_markup as show_markup, show_dep_ascii_markup as show_dep_markup\n",
    "from slovnet import Morph, Syntax\n",
    "from razdel import sentenize, tokenize\n",
    "\n",
    "from yargy import Parser, rule, and_, not_, or_\n",
    "from yargy.interpretation import fact\n",
    "from yargy.predicates import gram, tag, custom, type, in_\n",
    "from yargy.relations import gnc_relation\n",
    "from yargy.pipelines import morph_pipeline\n",
    "\n",
    "import pymorphy2\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "pymorphy = pymorphy2.MorphAnalyzer(lang='ru')\n",
    "\n",
    "navec = Navec.load('./data/navec_news_v1_1B_250K_300d_100q.tar')\n",
    "\n",
    "ner = NER.load('./data/slovnet_ner_news_v1.tar')\n",
    "morph = Morph.load('./data/slovnet_morph_news_v1.tar', batch_size=4)\n",
    "syntax = Syntax.load('./data/slovnet_syntax_news_v1.tar')\n",
    "\n",
    "ner.navec(navec)\n",
    "morph.navec(navec)\n",
    "syntax.navec(navec)\n",
    "\n",
    "print(\"load: ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt =\"\"\"Совладельцы сети магазинов фиксированных низких цен Fix Price Артем Хачатрян и Сергей Ломакин стали долларовыми миллиардерами после выхода компании на IPO (первичное публичное размещение акций). Об этом свидетельствуют подсчеты Forbes.\n",
    "Журнал оценил состояние каждого из них в 3,4 миллиарда долларов. Bloomberg написал о 3,6 миллиарда.\n",
    "Fix Price разместила свои бумаги на Лондонской бирже по цене 9,75 доллара за штуку. Ломакину и Хачатряну до размещения принадлежало по 354 067 500 акций (41,66%) компаний, сейчас — по 301 151 876 акций (35,43%). Таким образом, в ходе IPO каждый из них продал по 52,9 миллиона акций, и заработал по 516 миллионов долларов, подсчитал Forbes.\n",
    "Ранее сеть Fix Price, бумаги которой с 10 марта 2021 года начнут торговаться на Лондонской и Московской биржах, сообщила, что привлечет в ходе IPO 2 миллиарда долларов.\n",
    "«Интерфакс» писал, что это крупнейшее IPO российской компании с 2010 года, когда «Русал» в рамках первичного публичного размещения акций привлек 2,24 миллиарда долларов.\n",
    "Первые магазины сети Fix Price были открыты в 2007 году. Сейчас у компании 4279 магазинов в России, Беларуси, Казахстане, Узбекистане, Кыргызстане, Грузии и Латвии. Вначале весь ассортимент в Fix Price продавался по 30 рублей за товар, сейчас товары там стоят не дороже 250 рублей.\n",
    "Артем Хачатрян и Сергей Ломакин в 1998 году стали сооснователями сети дискаунтеров «Копейка» (сейчас эти магазины известны под брендом «Пятерочка»). В 2007 году они продали свою долю корпорации «Уралсиб» за 220 миллионов долларов. Также Хачатрян и Ломакин инвестировали в сети «Модис» и «ЦентрОбувь».\n",
    "В мае 2020 года Forbes писал, что в России больше 100 долларовых миллиардеров.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "На сайте экспертного центра НАТО «Атлантический совет» вышла нехарактерная для него статья.\n",
    "В ней авторы рекомендуют странам военного альянса применять к России более гибкую внешнюю политику, основанную не на очередных санкциях за нарушения прав человека, а на взаимных стратегических интересах.\n",
    "А демократизация России, говорится в статье, вообще может быть невыгодна США, ведь тот же Алексей Навальный — националист и поддерживал Кремль в крымском вопросе.\n",
    "Статья вызвала громкий скандал в экспертных кругах и ответные открытые письма с обвинениями авторов в коррупции.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "Компания «Яндекс» ведет переговоры о покупке небольшого банка «Акрополь», \n",
    "который принадлежит генеральному директору группы «Связной» Евгению \n",
    "Давыдовичу. Об этом сообщает The Bell со ссылкой на четыре источника \n",
    "на платежном и банковском рынке. \n",
    "Сделка, по данным издания, уже находится на одном из завершающих этапов. \n",
    "Договор купли-продажи может быть подписан в ближайшие несколько дней. Сумму\n",
    "сделки The Bell не приводит. \n",
    "Активы «Акрополя» составляют 1,24 миллиарда рублей. В рейтинге крупнейших \n",
    "банков «Интерфакс-100» он занимает 323-ю строчку.\n",
    "Евгений Давыдович отказался от комментариев. Представитель «Яндекса» \n",
    "сообщил The Bell, что компании «интересно получение банковской лицензии». \n",
    "«Мы рассматриваем все доступные варианты», — добавил он.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "Режиссер фильма «Супернова» Гарри Макквин подверг критике действия \n",
    "прокатчика картины в России, вырезавшего сцену, где гей-пара пытается заняться \n",
    "сексом. Заявление режиссера приводит издание The Advocate. \n",
    "Мы, создатели фильма, самым решительным образом возражаем против цензуры\n",
    "«Суперновы» в России. Вызывает глубокую обеспокоенность факт того, что \n",
    "фильм был смонтирован против нашей воли и без нашего разрешения.\n",
    "Макквин подчеркнул, что команда создателей фильма понимает, что \n",
    "на дистрибьютора оказывалось давление, но «не потерпит цензуры такого рода». \n",
    "В самой компании World Pictures, занимающейся распространением фильма \n",
    "в России, от комментариев отказались. \n",
    "Фильм «Супернова» рассказывает о жизни гей-пары, в которой один \n",
    "из партнеров теряет память из-за деменции. Из российской версии картины был \n",
    "вырезан трехминутный эпизод, в котором мужчины пытаются заняться сексом. \n",
    "В 2019 году в российском прокате похожей цензуре был подвергнут фильм \n",
    "«Рокетмен», снятый по мотивам биографии певца Элтона Джона. В РФ фильм \n",
    "показывался без сцены секса двух мужчин и финальных титров, в которых \n",
    "говорилось, что сейчас певец живет в браке с мужчиной. Создатели фильма \n",
    "и сам Элтон Джон подвергли это решение критике.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "Международный спортивный арбитраж (CAS) в Лозанне отклонил запрос \n",
    "Олимпийского комитета России, предлагавшего использовать песню «Катюша» \n",
    "вместо российского гимна на Олимпийских играх в Токио и Пекине.\n",
    "«Комиссия CAS считает, что [понятие] „любой гимн, связанный с Россией“, \n",
    "распространяется на любую песню, связанную с Россией или со ссылками на нее, \n",
    "в том числе с „Катюшей“», — сказано в решении суда, которое цитирует Associated \n",
    "Press.\n",
    "В декабре 2019 года Всемирное антидопинговое агентство отстранило Россию \n",
    "от Олимпийских и Паралимпийских игр, а также чемпионатов мира из-за \n",
    "манипуляций с допинг-пробами, которые содержались в базе данных \n",
    "Московской антидопинговой лаборатории.\n",
    "Всемирное антидопинговое агентство планировало отстранить Россию \n",
    "от международных соревнований на четыре года, но в декабре 2020 года \n",
    "спортивный арбитраж решил сократить срок наказания до двух лет. Таким \n",
    "образом, российские спортсмены не смогут до 16 декабря 2022 года выступать \n",
    "под национальным флагом, а также использовать национальный гимн и название \n",
    "сборной России.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "Форум проводит организация «Объединенные демократы», которая \n",
    "не является «нежелательной». По данным «Дождя», задержания мотивированы \n",
    "деятельностью «Открытой России», которая, по меньшей мере формально, \n",
    "не имеет отношения к проведению форума. Зарегистрированная в Великобритании \n",
    "организация с таким названием была еще в 2017 году признана \n",
    "в РФ «нежелательной», после чего началось преследование членов одноименного \n",
    "движения «Открытая Россия». В 2019 году оно объявило о ликвидации, и активисты\n",
    "этой организации учредили другую структуру с аналогичным названием, \n",
    "но не получили регистрацию в Минюсте. «Обычно, раньше по крайней мере, \n",
    "приходили по статье о сотрудничестве с нежелательной организацией \n",
    "на мероприятия „Открытой России“. Каким образом полиция связала „Открытку“ \n",
    "и „Объединенных демократов“, я не знаю. Это абсолютно разные организации», — \n",
    "сказала координатор «Открытой России» Татьяна Усманова. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "Первые упоминания о невзаимозаменяемых токенах, они же NFT, появились в 2017 \n",
    "году — тогда, например, проект Larva Labs запустил эксперимент под названием \n",
    "CryptoPunks. Он представлял собой коллекцию из 10 тысяч цифровых аватаров — \n",
    "по сути, графических изображений разных лиц, всякое из которых уникально, \n",
    "то есть найти две похожие картинки невозможно. При этом каждое изображение \n",
    "«привязали» к фрагменту компьютерного кода в блокчейн-платформе Ethereum — \n",
    "таким образом, у него был уникальный токен, в котором, в частности, хранилась \n",
    "информация о владельце изображения.\n",
    "В итоге скачать эту картинку, как и любое другое изображение в интернете, мог кто \n",
    "угодно, однако ее единственным официальным владельцем являлся именно тот, чей\n",
    "Ethereum-кошелек был указан в NFT к этой картинке. \n",
    "В чем смысл обладания изображением, которое любой человек может \n",
    "беспрепятственно скачать в интернете и, например, сделать своей аватаркой \n",
    "в соцсетях? Понять это можно, лишь рассмотрев эти аватары как произведения \n",
    "искусства и, в частности, проведя параллели между ними и «материальными» \n",
    "объектами вроде картин или скульптур. Репродукцию «Девочки с персиками» тоже \n",
    "может повесить у себя на кухне любой человек, но существует оригинал, который \n",
    "хранится в Третьяковской галерее, и у него имеется владелец. Чем популярнее \n",
    "становится картина (в том числе, чем больше ее репродукций «пойдет в народ»), \n",
    "тем выше может быть цена на оригинал.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_txt = \"\"\"\n",
    "Выгорание 🔥🚬\n",
    "\n",
    "Сегодня общались с коллегой на тему выгорания. Если вам кажется, что вы ни разу не сталкивались с этим, то скорее всего вы просто не доходили до критических стадий.\n",
    "\n",
    "Выгорание свойственно любой профессии, но особенно часто встречается в it сфере. Проблема серьезная и разбирать каждый отдельный случай должны специалисты, но представлять, что это и как не довести себя до критических стадий, нужно каждому. Как-то раз я смотрел очень полезную лекцию на эту тему, поделюсь тем, что запомнил и что применяю сам.\n",
    "\n",
    "Вам когда-нибудь казалось, что вы не получаете удовольствие от работы? Что не хочется делать рабочие задачи и даже то, что раньше в работе вас радовало, кажется каким-то скучным и даже неприятным? А цели у вас просто нет и вы живёте в дне сурка, где нет ни единого просвета? Так проявляются начальные стадии выгорания. Если не обратить на это должного внимания, то дальше все только усугубится.\n",
    "\n",
    "Сама проблема заключается в постепенном угасании «энергии», которой в конечном итоге хватает только на лежание на диване. На самом деле есть научное обоснование, связанное с выработкой нейромедиаторов - дофамина, серотонина и прочих. Лучше загуглить, но я расскажу вкратце. Дофамин отвечает за вашу мотивацию и самое главное - удовольствие от выполненной задачи. Получается такой «наркотик» - вы хотите что-то крутое сделать, чтобы испытать удовлетворение. Если уровни нейромедиаторов снижаются, человек впадает в депрессию.\n",
    "\n",
    "Разделяют 4 стадии выгорания. На первой и второй выгорание совсем незаметно, его легко спутать с усталостью. Третья стадия уже выделяется и вы можете понять - да, я выгорел, мне не нравится моя деятельность. А на четвёртой не хочется делать абсолютно ничего, полная апатия и отвращение ко всему. Хорошая новость в том, что из первых трёх стадий можно выйти самостоятельно.\n",
    "\n",
    "А как выйти из такого состояния, расскажем в следующем посте 😉\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Token:\n",
    "    def __init__(self, token):\n",
    "        self.token = token\n",
    "        self.morph = None\n",
    "        self.syntax = None\n",
    "    \n",
    "    def set_morph(self, morph):\n",
    "        self.morph = morph\n",
    "        \n",
    "    def set_syntax(self, syntax):\n",
    "        self.syntax = syntax\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.token\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.tokens = []\n",
    "        self.clean_tokens = []\n",
    "        self.syntax_tree = [] \n",
    "\n",
    "        self.tokenize()\n",
    "        self.clean()\n",
    "        \n",
    "    @property\n",
    "    def words(self):\n",
    "        return list(map(str, self.tokens))\n",
    "\n",
    "    def tokenize(self):\n",
    "        self.tokens = [Token(_.text) for _ in tokenize(self.sentence)]\n",
    "        \n",
    "    def clean(self):\n",
    "        clean_text = re.sub('[^а-яА-Яa-zA-Z]', ' ', self.sentence)\n",
    "        clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
    "        self.clean_tokens = [Token(_.text) for _ in tokenize(clean_text)]\n",
    "        \n",
    "    def morph(self):\n",
    "        markup = morph(self.words)\n",
    "\n",
    "        for i, token in enumerate(markup.tokens):\n",
    "            self.tokens[i].set_morph(token)\n",
    "            \n",
    "    def syntax(self):\n",
    "        markup = syntax(self.words)\n",
    "\n",
    "        for i, token in enumerate(markup.tokens):\n",
    "            self.tokens[i].set_syntax(token)\n",
    "\n",
    "            source = int(token.head_id) - 1\n",
    "            target = int(token.id) - 1\n",
    "\n",
    "            if source > 0 and source != target:  # skip root, loops\n",
    "                self.syntax_tree.append([source, target, token.rel])\n",
    "                \n",
    "    def show_syntax(self):\n",
    "        if self.syntax_tree == []:\n",
    "            self.syntax()\n",
    "\n",
    "        show_dep_markup(self.words, self.syntax_tree)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.sentence\n",
    "        \n",
    "    def __repr__(self):\n",
    "        s = ''\n",
    "        for tok in self.tokens:\n",
    "            s += '[{}] '.format(tok)\n",
    "        return s\n",
    "\n",
    "\n",
    "class Text:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "        self.sentences = []\n",
    "        \n",
    "        self.sentenize()\n",
    "        \n",
    "    def sentenize(self):\n",
    "        self.sentences = [Sentence(_.text) for _ in sentenize(self.text)]\n",
    "        \n",
    "    def morph_all_sentences(self):\n",
    "        for sent in self.sentences:\n",
    "            sent.morph()\n",
    "            \n",
    "    def syntax_all_sentences(self):\n",
    "        for sent in self.sentences:\n",
    "            sent.syntax()\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.text\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return '\\n\\n Sentence: '.join([str(sent) for sent in self.sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сегментация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Text(source_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Морфологический и синтаксический разбор "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.morph_all_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.syntax_all_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = text.sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌► Первые             amod\n",
      "  ┌►┌─┌───└─ упоминания         nsubj\n",
      "  │ │ │ ┌──► о                  case\n",
      "  │ │ │ │ ┌► невзаимозаменяемых amod\n",
      "  │ │ └►└─└─ токенах            nmod\n",
      "  │ │ ┌────► ,                  punct\n",
      "  │ │ │ ┌►┌─ они                nsubj\n",
      "  │ │ │ │ └► же                 advmod\n",
      "  │ │ │ └─┌► NFT                parataxis\n",
      "  │ └►│   │  ,                  punct\n",
      "┌─└─┌─└───└─ появились          \n",
      "│   │ │ ┌──► в                  case\n",
      "│   │ │ │ ┌► 2017               amod\n",
      "│   │ └►└─└─ году               obl\n",
      "│   │   ┌──► —                  punct\n",
      "│   └──►│    тогда              advmod\n",
      "│       │ ┌► ,                  punct\n",
      "│     ┌►└─└─ например           parataxis\n",
      "│     │ └──► ,                  punct\n",
      "│   ┌►└───┌─ проект             nsubj\n",
      "│   │   ┌─└► Larva              appos\n",
      "│   │   └──► Labs               flat:foreign\n",
      "│   └─────┌─ запустил           \n",
      "│       ┌─└► эксперимент        obj\n",
      "│       │ ┌► под                case\n",
      "│       └►└─ названием          nmod\n",
      "│       └──► CryptoPunks        appos\n",
      "└──────────► .                  punct\n"
     ]
    }
   ],
   "source": [
    "sentence.show_syntax()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Извлечение именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "markup = ner(text.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tex2jax_ignore\" style=\"white-space: pre-wrap\">\n",
       "Первые упоминания о невзаимозаменяемых токенах, они же NFT, появились в 2017 \n",
       "году — тогда, например, проект <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">Larva Labs<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">ORG</span></span> запустил эксперимент под названием \n",
       "CryptoPunks. Он представлял собой коллекцию из 10 тысяч цифровых аватаров — \n",
       "по сути, графических изображений разных лиц, всякое из которых уникально, \n",
       "то есть найти две похожие картинки невозможно. При этом каждое изображение \n",
       "«привязали» к фрагменту компьютерного кода в блокчейн-платформе Ethereum — \n",
       "таким образом, у него был уникальный токен, в котором, в частности, хранилась \n",
       "информация о владельце изображения.\n",
       "В итоге скачать эту картинку, как и любое другое изображение в интернете, мог кто \n",
       "угодно, однако ее единственным официальным владельцем являлся именно тот, чей\n",
       "Ethereum-кошелек был указан в NFT к этой картинке. \n",
       "В чем смысл обладания изображением, которое любой человек может \n",
       "беспрепятственно скачать в интернете и, например, сделать своей аватаркой \n",
       "в соцсетях? Понять это можно, лишь рассмотрев эти аватары как произведения \n",
       "искусства и, в частности, проведя параллели между ними и «материальными» \n",
       "объектами вроде картин или скульптур. Репродукцию «Девочки с персиками» тоже \n",
       "может повесить у себя на кухне любой человек, но существует оригинал, который \n",
       "хранится в <span style=\"padding: 2px; border-radius: 4px; border: 1px solid #bbdefb; background: #e3f2fd\">Третьяковской галерее<span style=\"vertical-align: middle; margin-left: 2px; font-size: 0.7em; color: #64b5f6;\">ORG</span></span>, и у него имеется владелец. Чем популярнее \n",
       "становится картина (в том числе, чем больше ее репродукций «пойдет в народ»), \n",
       "тем выше может быть цена на оригинал.\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_span_markup(markup.text, markup.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск словосочетаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_matches(rule, *lines):\n",
    "    parser = Parser(rule)\n",
    "    for line in lines:\n",
    "        matches = parser.findall(line)\n",
    "        matches = sorted(matches, key=lambda _: _.span)\n",
    "        spans = [_.span for _ in matches]\n",
    "        show_markup(line, spans)\n",
    "        if matches:\n",
    "            facts = [_.fact for _ in matches]\n",
    "            if len(facts) == 1:\n",
    "                facts = facts[0]\n",
    "            display(facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_gram(tag):\n",
    "    def check(v):\n",
    "        parse = pymorphy.parse(v)[0]\n",
    "        if not parse:\n",
    "            return False\n",
    "\n",
    "        return parse.tag.POS == tag\n",
    "    \n",
    "    return custom(check)\n",
    "\n",
    "INT = type('INT')\n",
    "NOUN = gram('NOUN')\n",
    "ADJF = gram('ADJF')\n",
    "ABBR = gram('Abbr')\n",
    "VERB = gram('VERB')\n",
    "PREP = gram('PREP')\n",
    "\n",
    "EQ_NOUN = exact_gram('NOUN')\n",
    "\n",
    "NUMBER = rule(\n",
    "    INT.optional(),\n",
    "    in_('.,').optional(),\n",
    "    INT\n",
    ")\n",
    "\n",
    "gnc = gnc_relation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "NounPhrase = fact('NounPhrase', ['adj', 'noun'])\n",
    "\n",
    "NumberPhrase = fact('NumberPhrase', ['number', 'noun'])\n",
    "\n",
    "Fact = fact('Fact', ['value'])\n",
    "\n",
    "NounPhraseRule = rule(\n",
    "    ADJF.optional().repeatable().interpretation(\n",
    "        NounPhrase.adj\n",
    "    ).match(gnc),\n",
    "\n",
    "    EQ_NOUN.repeatable().interpretation(\n",
    "        NounPhrase.noun\n",
    "    ).match(gnc)\n",
    ").interpretation(NounPhrase).interpretation(Fact.value)\n",
    "\n",
    "NumberPhraseRule = rule(\n",
    "    NUMBER.interpretation(\n",
    "        NumberPhrase.number\n",
    "    ),\n",
    "    EQ_NOUN.interpretation(\n",
    "        NumberPhrase.noun\n",
    "    )\n",
    ").interpretation(NumberPhrase).interpretation(Fact.value)\n",
    "\n",
    "AllRules = rule(or_(\n",
    "    NumberPhraseRule,\n",
    "    NounPhraseRule\n",
    ")).interpretation(Fact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые упоминания о невзаимозаменяемых токенах, они же NFT, появились \n",
      "─────────────────   ──────────────────────────                        \n",
      "в 2017 \n",
      "году — тогда, например, проект Larva Labs запустил эксперимент под \n",
      "────                    ──────                     ───────────     \n",
      "названием \n",
      "───────── \n",
      "CryptoPunks.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Fact(\n",
       "     value=NounPhrase(\n",
       "         adj='Первые',\n",
       "         noun='упоминания'\n",
       "     )\n",
       " ),\n",
       " Fact(\n",
       "     value=NounPhrase(\n",
       "         adj='невзаимозаменяемых',\n",
       "         noun='токенах'\n",
       "     )\n",
       " ),\n",
       " Fact(\n",
       "     value=NounPhrase(\n",
       "         adj=None,\n",
       "         noun='году'\n",
       "     )\n",
       " ),\n",
       " Fact(\n",
       "     value=NounPhrase(\n",
       "         adj=None,\n",
       "         noun='проект'\n",
       "     )\n",
       " ),\n",
       " Fact(\n",
       "     value=NounPhrase(\n",
       "         adj=None,\n",
       "         noun='эксперимент'\n",
       "     )\n",
       " ),\n",
       " Fact(\n",
       "     value=NounPhrase(\n",
       "         adj=None,\n",
       "         noun='названием'\n",
       "     )\n",
       " )]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_matches(AllRules, text.sentences[0].sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Построение короткого предложения по дереву"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_token_head(sentence, token):\n",
    "    return next((t for t in sentence.tokens if t.syntax.id == token.syntax.head_id), None)\n",
    "\n",
    "def find_token_children(sentence, token):\n",
    "    return list(filter(lambda t: t.syntax.head_id == token.syntax.id, sentence.tokens))\n",
    "\n",
    "def distance_from_root(sentence, token, was_here=[]):\n",
    "    if token.syntax.rel in ['root', 'ccomp', 'xcomp', 'acl', 'acl:relcl', 'parataxis', 'advcl'] or not token or token in was_here:\n",
    "        return 0\n",
    "    \n",
    "    append = 1\n",
    "    \n",
    "    if token.syntax.rel in ['conj', 'obj', '' 'cc']:\n",
    "        append = 0\n",
    "    \n",
    "    return append + distance_from_root(sentence, find_token_head(sentence, token), was_here + [token])\n",
    "\n",
    "def shorten(sentence, percent=0.5, show_syntax=True):\n",
    "    filtered_tokens = []\n",
    "    \n",
    "    required_filter = lambda t: t.syntax.rel in ['nsubj', 'obj', 'root']\n",
    "    \n",
    "    nummod_filter = lambda t: t.syntax.rel.startswith('nummod')\n",
    "    advmod_filter = lambda t: t.syntax.rel.startswith('advmod')\n",
    "    case_filter = lambda t: t.syntax.rel == 'case'\n",
    "    empty_modifier_filter = lambda t: t.syntax.rel in ['nmod', 'amod'] and len(find_token_children(sentence, t)) == 0\n",
    "    discourse_filter = lambda t: t.syntax.rel in ['discourse']\n",
    "    \n",
    "    if show_syntax:\n",
    "        sentence.show_syntax()\n",
    "\n",
    "    root = next((t for t in sentence.tokens if t.syntax.rel == 'root'), None)\n",
    "    \n",
    "    if not root:\n",
    "        return\n",
    "    \n",
    "    tokens_distance = [(t, distance_from_root(sentence, t)) for t in sentence.tokens]\n",
    "    max_depth = max(t[1] for t in tokens_distance)\n",
    "    max_allowed_token_depth = max_depth * percent\n",
    "\n",
    "    was_changed = True\n",
    "    \n",
    "    # TODO тут наверно стоит все-таки проходиться от корней, чтобы корректно учитывать, какие токены включены, какие нет.\n",
    "    # Иначе из-за порядка обхода теряется часть токенов (родитель добавляется позже ребенка и некоторые правила для ребенка не срабатывают)\n",
    "    # Уже переделал на while, чтобы не переписывать логику обхода. Можно потом переписать более оптимально\n",
    "    \n",
    "    # TODO еще стоит разделять предложения на составные части (сложносочиненные/подчиненные),\n",
    "    # чтобы учитывать макс. дистанцию от корня в конкретной ветке предложения\n",
    "\n",
    "    i = -1\n",
    "    while was_changed:\n",
    "        i += 1\n",
    "        was_changed = False\n",
    "\n",
    "        def push_to_result(t):\n",
    "            nonlocal was_changed\n",
    "            if t.syntax.id not in map(lambda t: t.syntax.id, filtered_tokens):\n",
    "                filtered_tokens.append(t)\n",
    "                was_changed = True\n",
    "\n",
    "        for t, d in tokens_distance:\n",
    "            t_head = find_token_head(sentence, t)\n",
    "            t_head_included = t_head and t_head.syntax.id in map(lambda t: t.syntax.id, filtered_tokens)\n",
    "\n",
    "            # берем только top-N слов от корня\n",
    "            # исключаем \"висячие\" модификаторы, они могут быть включены по другим правилам ниже\n",
    "            if (d <= max_allowed_token_depth or required_filter(t)) and \\\n",
    "                not empty_modifier_filter(t) and \\\n",
    "                not discourse_filter(t):\n",
    "                push_to_result(t)\n",
    "\n",
    "            if t_head_included and t_head:\n",
    "                # берем все числа, чтобы не потерять смысл\n",
    "                if nummod_filter(t):\n",
    "                    push_to_result(t)\n",
    "                \n",
    "                # на <- IPO, nmod -> case. Чтобы предложение оставалось читаемым, оставляем предлоги, чтобы не было висячих существительных\n",
    "                if case_filter(t):\n",
    "                    push_to_result(t)\n",
    "\n",
    "                # выхода -> компании, obl -> any\n",
    "                if t_head.syntax.rel == 'obl':\n",
    "                    push_to_result(t)\n",
    "\n",
    "                # не <- дороже, advmod -> advmod\n",
    "                if t_head.syntax.rel == t.syntax.rel and t.syntax.rel in ['advmod', 'amod']:\n",
    "                    push_to_result(t)\n",
    "\n",
    "                # берем существительные модификаторы к объектам и субъектам предложения\n",
    "                if t_head.syntax.rel in ['nsubj', 'obj'] and t.syntax.rel in ['nmod']:\n",
    "                    push_to_result(t)\n",
    "    \n",
    "    print(i + 1, 'iterations')\n",
    "    \n",
    "    filtered_tokens.sort(key=lambda t: int(t.syntax.id))\n",
    "\n",
    "    return list(map(lambda t: t.token, filtered_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          ┌► Первые             amod\n",
      "  ┌►┌─┌───└─ упоминания         nsubj\n",
      "  │ │ │ ┌──► о                  case\n",
      "  │ │ │ │ ┌► невзаимозаменяемых amod\n",
      "  │ │ └►└─└─ токенах            nmod\n",
      "  │ │ ┌────► ,                  punct\n",
      "  │ │ │ ┌►┌─ они                nsubj\n",
      "  │ │ │ │ └► же                 advmod\n",
      "  │ │ │ └─┌► NFT                parataxis\n",
      "  │ └►│   │  ,                  punct\n",
      "┌─└─┌─└───└─ появились          \n",
      "│   │ │ ┌──► в                  case\n",
      "│   │ │ │ ┌► 2017               amod\n",
      "│   │ └►└─└─ году               obl\n",
      "│   │   ┌──► —                  punct\n",
      "│   └──►│    тогда              advmod\n",
      "│       │ ┌► ,                  punct\n",
      "│     ┌►└─└─ например           parataxis\n",
      "│     │ └──► ,                  punct\n",
      "│   ┌►└───┌─ проект             nsubj\n",
      "│   │   ┌─└► Larva              appos\n",
      "│   │   └──► Labs               flat:foreign\n",
      "│   └─────┌─ запустил           \n",
      "│       ┌─└► эксперимент        obj\n",
      "│       │ ┌► под                case\n",
      "│       └►└─ названием          nmod\n",
      "│       └──► CryptoPunks        appos\n",
      "└──────────► .                  punct\n",
      "3 iterations\n",
      "Первые упоминания о невзаимозаменяемых токенах, они же NFT, появились в 2017 \n",
      "году — тогда, например, проект Larva Labs запустил эксперимент под названием \n",
      "CryptoPunks. 28\n",
      "упоминания о токенах , они NFT появились в 2017 году — тогда , например , проект запустил эксперимент под названием . 21\n",
      "-----------\n",
      "\n",
      "\n",
      "                ┌► Он          nsubj\n",
      "┌─┌─────────┌─┌─└─ представлял \n",
      "│ │         │ └──► собой       fixed\n",
      "│ │     ┌─┌─└────► коллекцию   obj\n",
      "│ │     │ │ ┌────► из          case\n",
      "│ │     │ │ │   ┌► 10          nummod\n",
      "│ │     │ │ │ ┌►└─ тысяч       nummod\n",
      "│ │     │ │ │ │ ┌► цифровых    amod\n",
      "│ │     │ └►└─└─└─ аватаров    nmod\n",
      "│ │   ┌►│          —           punct\n",
      "│ │   │ │       ┌► по          case\n",
      "│ │ ┌►│ │     ┌─└─ сути        parataxis\n",
      "│ │ │ │ │     └──► ,           punct\n",
      "│ │ │ │ │       ┌► графических amod\n",
      "│ │ │ │ └────►┌─└─ изображений nmod\n",
      "│ │ │ │       │ ┌► разных      amod\n",
      "│ │ │ │       └►└─ лиц         nmod\n",
      "│ │ │ │   ┌──────► ,           punct\n",
      "│ │ │ │   │ ┌►┌─── всякое      nsubj\n",
      "│ │ │ │   │ │ │ ┌► из          case\n",
      "│ │ │ │   │ │ └►└─ которых     nmod\n",
      "│ │ │ └───└─└───── уникально   \n",
      "│ │ │ ┌──────────► ,           punct\n",
      "│ │ │ │ ┌──────►┌─ то          mark\n",
      "│ │ │ │ │       └► есть        fixed\n",
      "│ │ │ │ │ ┌►┌───── найти       csubj\n",
      "│ │ │ │ │ │ │ ┌──► две         nummod:gov\n",
      "│ │ │ │ │ │ │ │ ┌► похожие     amod\n",
      "│ │ │ │ │ │ └►└─└─ картинки    obj\n",
      "│ └►└─└─└─└─────── невозможно  conj\n",
      "└────────────────► .           punct\n",
      "4 iterations\n",
      "Он представлял собой коллекцию из 10 тысяч цифровых аватаров — \n",
      "по сути, графических изображений разных лиц, всякое из которых уникально, \n",
      "то есть найти две похожие картинки невозможно. 31\n",
      "Он представлял собой коллекцию из 10 тысяч аватаров — по сути , изображений , всякое из которых уникально , то найти две картинки невозможно . 25\n",
      "-----------\n",
      "\n",
      "\n",
      "                    ┌► При                case\n",
      "┌──────────────────►└─ этом               obl\n",
      "│                   ┌► каждое             det\n",
      "│                 ┌►└─ изображение        nsubj\n",
      "│                 │ ┌► «                  punct\n",
      "│ ┌──────────►┌─┌─└─└─ привязали          acl:relcl\n",
      "│ │           │ │ └──► »                  punct\n",
      "│ │           │ │   ┌► к                  case\n",
      "│ │           │ └►┌─└─ фрагменту          obl\n",
      "│ │           │   │ ┌► компьютерного      amod\n",
      "│ │           │   └►└─ кода               nmod\n",
      "│ │           │     ┌► в                  case\n",
      "│ │           └──►┌─└─ блокчейн-платформе obl\n",
      "│ │               └──► Ethereum           appos\n",
      "│ │           ┌──────► —                  punct\n",
      "│ │           │     ┌► таким              det\n",
      "│ │   ┌──────►│     └─ образом            parataxis\n",
      "│ │   │ ┌────►│        ,                  punct\n",
      "│ │   │ │     │     ┌► у                  case\n",
      "│ │   │ │     │   ┌►└─ него               obl\n",
      "└─│ ┌─│ │     │ ┌─└─── был                \n",
      "  │ │ │ │     │ │   ┌► уникальный         amod\n",
      "  └─│ │ │     └─└──►└─ токен              nsubj\n",
      "    │ │ │ ┌──────────► ,                  punct\n",
      "    │ │ │ │         ┌► в                  case\n",
      "    │ │ └─│ ┌──────►└─ котором            obl\n",
      "    │ │   │ │       ┌► ,                  punct\n",
      "    │ │   │ │ ┌►┌─┌─└─ в                  discourse\n",
      "    │ │   │ │ │ │ └──► частности          fixed\n",
      "    │ │   │ │ │ └────► ,                  punct\n",
      "    │ └───└─└─└─────┌─ хранилась          \n",
      "    │             ┌─└► информация         nsubj\n",
      "    │             │ ┌► о                  case\n",
      "    │             └►└─ владельце          nmod\n",
      "    │             └──► изображения        nmod\n",
      "    └────────────────► .                  punct\n",
      "3 iterations\n",
      "При этом каждое изображение \n",
      "«привязали» к фрагменту компьютерного кода в блокчейн-платформе Ethereum — \n",
      "таким образом, у него был уникальный токен, в котором, в частности, хранилась \n",
      "информация о владельце изображения. 36\n",
      "При этом изображение « привязали » к фрагменту кода в блокчейн-платформе Ethereum таким образом , у него был токен , в котором хранилась информация о владельце . 27\n",
      "-----------\n",
      "\n",
      "\n",
      "                      ┌► В                case\n",
      "  ┌──────────────────►└─ итоге            obl\n",
      "┌─│   ┌►┌───┌───────┌─── скачать          xcomp\n",
      "│ │   │ │   │       │ ┌► эту              det\n",
      "│ │   │ │   │       └►└─ картинку         obj\n",
      "│ │   │ │   │ ┌────────► ,                punct\n",
      "│ │   │ │   │ │ ┌──────► как              case\n",
      "│ │   │ │   │ │ │ ┌────► и                advmod\n",
      "│ │   │ │   │ │ │ │ ┌──► любое            det\n",
      "│ │   │ │   │ │ │ │ │ ┌► другое           amod\n",
      "│ │   │ │ ┌─└►└─└─└─└─└─ изображение      obj\n",
      "│ │   │ │ │           ┌► в                case\n",
      "│ │   │ └►│           └─ интернете        obl\n",
      "│ │ ┌►│   │              ,                punct\n",
      "│ │ │ └─┌─│     ┌─┌───┌─ мог              \n",
      "│ │ │   │ │     │ │ ┌─└► кто              nsubj\n",
      "│ │ │   │ │     │ │ └──► угодно           fixed\n",
      "│ │ │   │ │     │ └────► ,                punct\n",
      "│ │ │   │ │     └──────► однако           advmod\n",
      "│ │ │   │ └────────────► ее               det\n",
      "│ │ │   │           ┌──► единственным     amod\n",
      "│ │ │   │           │ ┌► официальным      amod\n",
      "│ │ │   │           └─└─ владельцем       xcomp\n",
      "│ └─│ ┌─│           └─── являлся          \n",
      "└──►│ │ │                именно           advmod\n",
      "    │ │ └──────►┌─────── тот              nsubj\n",
      "    │ │         │ ┌────► ,                punct\n",
      "    │ │         │ │   ┌► чей              det\n",
      "    │ │         │ │ ┌►└─ Ethereum-кошелек nsubj:pass\n",
      "    │ │         │ │ │ ┌► был              aux:pass\n",
      "    └─│       ┌─└►└─└─└─ указан           acl:relcl\n",
      "      │       │   │   ┌► в                case\n",
      "      │       │   └──►└─ NFT              obl\n",
      "      │       │     ┌──► к                case\n",
      "      │       │     │ ┌► этой             det\n",
      "      │       └────►└─└─ картинке         obl\n",
      "      └────────────────► .                punct\n",
      "3 iterations\n",
      "В итоге скачать эту картинку, как и любое другое изображение в интернете, мог кто \n",
      "угодно, однако ее единственным официальным владельцем являлся именно тот, чей\n",
      "Ethereum-кошелек был указан в NFT к этой картинке. 37\n",
      "В итоге скачать эту картинку , как и любое изображение в интернете , мог кто , однако ее владельцем являлся именно тот , Ethereum-кошелек был указан в NFT к этой картинке . 32\n",
      "-----------\n",
      "\n",
      "\n",
      "              ┌► В                case\n",
      "┌─┌─┌──────►┌─└─ чем              obl\n",
      "│ │ │       └──► смысл            nsubj\n",
      "│ │ │       ┌─┌► обладания        amod\n",
      "│ │ │   ┌───└►└─ изображением     nmod\n",
      "│ │ │ ┌►│        ,                punct\n",
      "│ │ │ │ │ ┌────► которое          nsubj\n",
      "│ │ │ │ │ │   ┌► любой            det\n",
      "│ │ │ │ │ │ ┌►└─ человек          nsubj\n",
      "│ │ │ │ └►└─└─── может            acl:relcl\n",
      "│ │ │ │   │   ┌► беспрепятственно advmod\n",
      "│ │ │ │ ┌─└──►└─ скачать          xcomp\n",
      "│ │ │ │ │     ┌► в                case\n",
      "│ │ │ │ └────►└─ интернете        obl\n",
      "│ │ │ │ ┌──────► и                cc\n",
      "│ │ │ │ │ ┌────► ,                punct\n",
      "│ │ │ │ │ │ ┌►┌─ например         parataxis\n",
      "│ │ │ │ │ │ │ └► ,                punct\n",
      "│ └►└─└─└─└─└─── сделать          csubj\n",
      "│     │   │   ┌► своей            det\n",
      "│     │   └──►└─ аватаркой        obj\n",
      "│     │       ┌► в                case\n",
      "│     └──────►└─ соцсетях         obl\n",
      "└──────────────► ?                punct\n",
      "                ┌► Понять        csubj\n",
      "                │  это           \n",
      "┌─┌─────────────└─ можно         \n",
      "│ │           ┌──► ,             punct\n",
      "│ │           │ ┌► лишь          advmod\n",
      "│ │ ┌───────┌─└─└─ рассмотрев    \n",
      "│ │ │       │   ┌► эти           det\n",
      "│ │ │       └──►└─ аватары       obj\n",
      "│ │ │           ┌► как           case\n",
      "│ │ │   ┌────►┌─└─ произведения  nmod\n",
      "│ │ │   │     └──► искусства     nmod\n",
      "│ │ │ ┌►│          и             cc\n",
      "│ │ │ │ │       ┌► ,             punct\n",
      "│ │ │ │ │ ┌►┌─┌─└─ в             discourse\n",
      "│ │ │ │ │ │ │ └──► частности     fixed\n",
      "│ │ │ │ │ │ └────► ,             punct\n",
      "│ └►│ │ │ └─────── проведя       advcl\n",
      "│   └►│ └─────┌─── параллели     obj\n",
      "│     │       │ ┌► между         case\n",
      "│     │   ┌─┌─└►└─ ними          nmod\n",
      "│     │   │ │ ┌──► и             cc\n",
      "│     │   │ │ │ ┌► «             punct\n",
      "│     └───│ └►└─└─ материальными conj\n",
      "│         │   └──► »             punct\n",
      "│         └──►┌─── объектами     conj\n",
      "│             │ ┌► вроде         case\n",
      "│           ┌─└►└─ картин        nmod\n",
      "│           │   ┌► или           cc\n",
      "│           └──►└─ скульптур     conj\n",
      "└────────────────► .             punct\n",
      "3 iterations\n",
      "Понять это можно, лишь рассмотрев эти аватары как произведения \n",
      "искусства и, в частности, проведя параллели между ними и «материальными» \n",
      "объектами вроде картин или скульптур. 30\n",
      "Понять это можно , лишь рассмотрев эти аватары как произведения и проведя параллели между ними и материальными объектами . 19\n",
      "-----------\n",
      "\n",
      "\n",
      "        ┌──────► Репродукцию   nsubj\n",
      "        │     ┌► «             punct\n",
      "        │ ┌─┌─└─ Девочки       \n",
      "        │ │ │ ┌► с             case\n",
      "        │ │ └►└─ персиками     nmod\n",
      "        │ └────► »             punct\n",
      "        │     ┌► тоже          advmod\n",
      "┌─┌─┌───└───┌─└─ может         \n",
      "│ │ │   ┌─┌─└──► повесить      xcomp\n",
      "│ │ │   │ │   ┌► у             case\n",
      "│ │ │   │ └──►└─ себя          obl\n",
      "│ │ │   │     ┌► на            case\n",
      "│ │ │ ┌►│     └─ кухне         obl\n",
      "│ │ │ │ │     ┌► любой         det\n",
      "│ │ │ │ └────►└─ человек       obj\n",
      "│ │ │ │     ┌──► ,             punct\n",
      "│ │ │ │     │ ┌► но            cc\n",
      "│ │ └►│     └─└─ существует    conj\n",
      "│ │   │   ┌─└──► оригинал      nsubj\n",
      "│ │   │   │ ┌──► ,             punct\n",
      "│ │   │   │ │ ┌► который       nsubj:pass\n",
      "│ │   └─┌─└►└─└─ хранится      acl:relcl\n",
      "│ │     │   ┌──► в             case\n",
      "│ │     │   │ ┌► Третьяковской amod\n",
      "│ │     └──►└─└─ галерее       obl\n",
      "│ │     ┌──────► ,             punct\n",
      "│ │     │ ┌────► и             cc\n",
      "│ │     │ │   ┌► у             case\n",
      "│ │     │ │ ┌►└─ него          obl\n",
      "│ └────►└─└─└─┌─ имеется       conj\n",
      "│             └► владелец      nsubj\n",
      "└──────────────► .             punct\n",
      "3 iterations\n",
      "Репродукцию «Девочки с персиками» тоже \n",
      "может повесить у себя на кухне любой человек, но существует оригинал, который \n",
      "хранится в Третьяковской галерее, и у него имеется владелец. 32\n",
      "Репродукцию Девочки тоже может повесить у себя на кухне любой человек , но существует оригинал , который хранится в Третьяковской галерее , и у него имеется владелец . 28\n",
      "-----------\n",
      "\n",
      "\n",
      "                ┌──► Чем         mark\n",
      "                │ ┌► популярнее  xcomp\n",
      "┌─┌─────────────└─└─ становится  \n",
      "│ │     ┌───────└──► картина     nsubj\n",
      "│ │     │ ┌────────► (           punct\n",
      "│ │     │ │     ┌──► в           case\n",
      "│ │     │ │     │ ┌► том         det\n",
      "│ │ ┌───│ │ ┌──►└─└─ числе       parataxis\n",
      "│ │ │   │ │ │   ┌──► ,           punct\n",
      "│ │ │   │ │ │   │ ┌► чем         mark\n",
      "│ │ │   │ │ │ ┌►└─└─ больше      advmod\n",
      "│ │ │   │ │ │ │   ┌► ее          det\n",
      "│ │ │   │ │ └─│ ┌►└─ репродукций obl\n",
      "│ │ │   │ │   │ │ ┌► «           punct\n",
      "│ │ │ ┌─└►└─┌─└─└─└─ пойдет      parataxis\n",
      "│ │ │ │ │ │ │ │   ┌► в           case\n",
      "│ │ │ │ │ │ │ └──►└─ народ       obl\n",
      "│ │ │ │ │ │ └──────► »           punct\n",
      "│ │ │ │ │ └────────► )           punct\n",
      "│ │ │ │ │         ┌► ,           punct\n",
      "│ │ └►│ │         │  тем         mark\n",
      "│ │   │ └────────►│  выше        obl\n",
      "│ └──►│           └─ может       parataxis\n",
      "│     └──────────►│  быть        cop\n",
      "│               ┌─└► цена        nsubj\n",
      "│               │ ┌► на          case\n",
      "│               └►└─ оригинал    nmod\n",
      "└──────────────────► .           punct\n",
      "3 iterations\n",
      "Чем популярнее \n",
      "становится картина (в том числе, чем больше ее репродукций «пойдет в народ»), \n",
      "тем выше может быть цена на оригинал. 28\n",
      "Чем популярнее становится картина ( в том числе больше ее репродукций « пойдет в народ » ) , тем выше может быть цена на оригинал . 26\n",
      "-----------\n",
      "\n",
      "\n",
      "src len 222 short 178 diff 44\n",
      "80%\n"
     ]
    }
   ],
   "source": [
    "total_src_len = 0\n",
    "total_short_len = 0\n",
    "\n",
    "for sent in text.sentences:\n",
    "    short = shorten(sent, 0.5, True)\n",
    "    \n",
    "    if not short:\n",
    "        continue\n",
    "    \n",
    "    src_len = len(sent.tokens)\n",
    "    short_len = len(short)\n",
    "    \n",
    "    print(sent, src_len)\n",
    "    print(' '.join(short), short_len)\n",
    "    print('-----------\\n\\n')\n",
    "    \n",
    "    total_short_len += short_len\n",
    "    total_src_len += src_len\n",
    "    \n",
    "print('src len', total_src_len, 'short', total_short_len, 'diff', total_src_len - total_short_len)\n",
    "print('{:.0f}%'.format(total_short_len / total_src_len * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summarizer import Summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(source_txt, num_sentences=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Первые упоминания о невзаимозаменяемых токенах, они же NFT, появились в 2017 \n",
      "году — тогда, например, проект Larva Labs запустил эксперимент под названием \n",
      "CryptoPunks. Он представлял собой коллекцию из 10 тысяч цифровых аватаров — \n",
      "по сути, графических изображений разных лиц, всякое из которых уникально, \n",
      "то есть найти две похожие картинки невозможно. При этом каждое изображение \n",
      "«привязали» к фрагменту компьютерного кода в блокчейн-платформе Ethereum — \n",
      "таким образом, у него был уникальный токен, в котором, в частности, хранилась \n",
      "информация о владельце изображения.\n",
      "В итоге скачать эту картинку, как и любое другое изображение в интернете, мог кто \n",
      "угодно, однако ее единственным официальным владельцем являлся именно тот, чей\n",
      "Ethereum-кошелек был указан в NFT к этой картинке. \n",
      "В чем смысл обладания изображением, которое любой человек может \n",
      "беспрепятственно скачать в интернете и, например, сделать своей аватаркой \n",
      "в соцсетях? Понять это можно, лишь рассмотрев эти аватары как произведения \n",
      "искусства и, в частности, проведя параллели между ними и «материальными» \n",
      "объектами вроде картин или скульптур. Репродукцию «Девочки с персиками» тоже \n",
      "может повесить у себя на кухне любой человек, но существует оригинал, который \n",
      "хранится в Третьяковской галерее, и у него имеется владелец. Чем популярнее \n",
      "становится картина (в том числе, чем больше ее репродукций «пойдет в народ»), \n",
      "тем выше может быть цена на оригинал.\n",
      "\n",
      "-----\n",
      "Первые упоминания о невзаимозаменяемых токенах, они же NFT, появились в 2017 \n",
      "году — тогда, например, проект Larva Labs запустил эксперимент под названием \n",
      "CryptoPunks. Он представлял собой коллекцию из 10 тысяч цифровых аватаров — \n",
      "по сути, графических изображений разных лиц, всякое из которых уникально, \n",
      "то есть найти две похожие картинки невозможно. Репродукцию «Девочки с персиками» тоже \n",
      "может повесить у себя на кухне любой человек, но существует оригинал, который \n",
      "хранится в Третьяковской галерее, и у него имеется владелец.\n"
     ]
    }
   ],
   "source": [
    "print(source_txt)\n",
    "\n",
    "print('-----')\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Частотная суммаризация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('russian')\n",
    "\n",
    "def freq_summarize(source_text, num_sentences=3):\n",
    "    text = Text(source_text)\n",
    "    \n",
    "    word_frequencies = {}\n",
    "    \n",
    "    for sentence in text.sentences:\n",
    "        for word in sentence.clean_tokens:\n",
    "            if word.token not in stopwords:\n",
    "                if word not in word_frequencies.keys():\n",
    "                    word_frequencies[word.token] = 1\n",
    "                else:\n",
    "                    word_frequencies[word.token] += 1\n",
    "    \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    sentence_scores = {}\n",
    "    for sentence in text.sentences:\n",
    "        for word in sentence.clean_tokens:\n",
    "            if word.token in word_frequencies.keys():\n",
    "                if sentence.sentence not in sentence_scores.keys():\n",
    "                    sentence_scores[sentence.sentence] = word_frequencies[word.token]\n",
    "                else:\n",
    "                    sentence_scores[sentence.sentence] += word_frequencies[word.token]\n",
    "                    \n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    \n",
    "    summary = ''\n",
    "    for sentence in text.sentences:\n",
    "        if sentence.sentence in summary_sentences:\n",
    "            summary += sentence.sentence + ' '\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он представлял собой коллекцию из 10 тысяч цифровых аватаров — \n",
      "по сути, графических изображений разных лиц, всякое из которых уникально, \n",
      "то есть найти две похожие картинки невозможно. При этом каждое изображение \n",
      "«привязали» к фрагменту компьютерного кода в блокчейн-платформе Ethereum — \n",
      "таким образом, у него был уникальный токен, в котором, в частности, хранилась \n",
      "информация о владельце изображения. В итоге скачать эту картинку, как и любое другое изображение в интернете, мог кто \n",
      "угодно, однако ее единственным официальным владельцем являлся именно тот, чей\n",
      "Ethereum-кошелек был указан в NFT к этой картинке. \n"
     ]
    }
   ],
   "source": [
    "print(freq_summarize(source_txt))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aspirant",
   "language": "python",
   "name": "aspirant"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
